{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import bernoulli\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "kDim = 10 # the number of dimension\n",
    "# kUsers = 610 # 610 is total, but we use 300 users \n",
    "# kMovies = 9724 # 9724 is total, but we use 6000 movies\n",
    "\n",
    "# These data are used to draw a kDim-vector that follows Gaussian Distribution (kMean, kCov)\n",
    "kMean = []\n",
    "for i in range(kDim):\n",
    "    kMean.append(0)\n",
    "kCov = 0.1 * np.identity(n=kDim,dtype='float')\n",
    "\n",
    "# Rating matrix, which is sparsew\n",
    "gR = dict() # Key = (userId, movieId)\n",
    "gD = dict() # Key = (userId, moveId) value = existence\n",
    "gU = dict() # Key = (userId), value = user vector\n",
    "gV = dict() # Key = (movieId), value = movie vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all model variables reading given file(csv)\n",
    "# It should be executed first before calling MakeTestSet which split the whole file into \n",
    "# training file and test file\n",
    "def GenerateModelVariables (in_file_name, in_U, in_V):\n",
    "    file = open(in_file_name, 'r')\n",
    "    fileReader = csv.reader(file)\n",
    "    \n",
    "    for row in fileReader:\n",
    "        if row[0] == 'userId':\n",
    "            continue\n",
    "        else:\n",
    "            currentUserID = int(row[0])\n",
    "            currentMovieID = int(row[1])\n",
    "            currentRating = float(row[2])\n",
    "\n",
    "            if type(in_U.get(currentUserID)) == type(None):\n",
    "                in_U[currentUserID] = np.random.multivariate_normal(kMean, kCov, 1).T\n",
    "            if type(gV.get(currentMovieID)) == type(None):\n",
    "                in_V[currentMovieID] = np.random.multivariate_normal(kMean, kCov, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test set... from given training file\n",
    "def MakeTestSet(in_file_name, in_ratio, out_trainingFileName, out_testFileName):\n",
    "    originalFile = open(in_file_name, 'r')\n",
    "    fileReader = csv.reader(originalFile)\n",
    "    \n",
    "    trainingFile = open(out_trainingFileName,'w')\n",
    "    testFile = open(out_testFileName, 'w')\n",
    "    \n",
    "    trainingCSVWriter = csv.writer(trainingFile, delimiter=',',quotechar=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    testCSVWriter = csv.writer(testFile, delimiter=',',quotechar=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    trainingCSVWriter.writerow(['userId', 'movieId', 'rating'])\n",
    "    testCSVWriter.writerow(['userId', 'movieId', 'rating'])\n",
    "    \n",
    "    for row in fileReader:\n",
    "        if row[0] == 'userId':\n",
    "            continue\n",
    "        else:\n",
    "            bTrainingSet = bernoulli.rvs(1 - in_ratio, size = 1)[0] # 1 indicates training data\n",
    "            if bTrainingSet == 1:\n",
    "                trainingCSVWriter.writerow([row[0], row[1], row[2]])\n",
    "            else:\n",
    "                testCSVWriter.writerow([row[0], row[1], row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose data in a set... from given file\n",
    "def ChooseSomeOfDataSet(in_file_name, in_ratio, out_trainingFileName):\n",
    "    originalFile = open(in_file_name, 'r')\n",
    "    fileReader = csv.reader(originalFile)\n",
    "    \n",
    "    trainingFile = open(out_trainingFileName,'w')\n",
    "    \n",
    "    trainingCSVWriter = csv.writer(trainingFile, delimiter=',',quotechar=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    trainingCSVWriter.writerow(['userId', 'movieId', 'rating'])\n",
    "    \n",
    "    for row in fileReader:\n",
    "        if row[0] == 'userId':\n",
    "            continue\n",
    "        else:\n",
    "            bTrainingSet = bernoulli.rvs(in_ratio, size = 1)[0] # 1 indicates training data\n",
    "            if bTrainingSet == 1:\n",
    "                trainingCSVWriter.writerow([row[0], row[1], row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training file\n",
    "# This function extract all information on the set of given ratings and existence\n",
    "def ReadTrainingCSVFile(in_file_name, out_D, out_R):\n",
    "    training_file = open(in_file_name, 'r')\n",
    "    fileReader = csv.reader(training_file)\n",
    "    \n",
    "    for row in fileReader:\n",
    "        if row[0] == 'userId':\n",
    "            continue\n",
    "        else:            \n",
    "            currentUserID = int(row[0])\n",
    "            currentMovieID = int(row[1])\n",
    "            currentRating = float(row[2])\n",
    "                        \n",
    "            out_D[(currentUserID, currentMovieID)] = 1\n",
    "            out_R[(currentUserID, currentMovieID)] = currentRating\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is used to verify that the objective function is minimizing\n",
    "def CalculateJointLikelihood(in_R, in_U, in_V, in_D):\n",
    "    \n",
    "    logLikelihood = 0.0\n",
    "    \n",
    "    for eachKey in in_D.keys():\n",
    "        logLikelihood = logLikelihood + ((in_R[eachKey] - (in_U[eachKey[0]].T @ in_V[eachKey[1]])) ** 2)\n",
    "    \n",
    "    return logLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We solve this problem iteratively. \n",
    "# For example, first set all U, V = 0\n",
    "#\n",
    "# For the first iteration:\n",
    "# Calculate U using given V\n",
    "# Next, \n",
    "# Calculate V using calculated U above.\n",
    "\n",
    "def CalculateMLE(in_init_U, in_init_V, in_D, in_R, in_nIterations):\n",
    "    \n",
    "    U = copy.deepcopy(in_init_U) # init U\n",
    "    V = copy.deepcopy(in_init_V) # init V\n",
    "    \n",
    "    for t in range(in_nIterations):\n",
    "        for i in U.keys():\n",
    "            firstTerm = np.zeros([kDim, kDim], float)\n",
    "            secondTerm = np.zeros([kDim, 1], float)\n",
    "\n",
    "            for j in V.keys():\n",
    "                if type(in_D.get((i,j))) != type(None):\n",
    "                    \n",
    "                    firstTerm = firstTerm + (V[j] @ V[j].T)\n",
    "                    secondTerm = secondTerm + (in_R[(i, j)] * V[j])\n",
    "            \n",
    "            U[i] = np.linalg.pinv(firstTerm) @ secondTerm\n",
    "\n",
    "        for j in V.keys():\n",
    "            firstTerm = np.zeros([kDim, kDim], float)\n",
    "            secondTerm = np.zeros([kDim, 1], float)\n",
    "\n",
    "            for i in U.keys():\n",
    "                if type(in_D.get((i,j))) != type(None):\n",
    "                    \n",
    "                    firstTerm = firstTerm + (U[i] @ U[i].T)\n",
    "                    secondTerm = secondTerm + (in_R[(i, j)] * U[i])\n",
    "            \n",
    "            V[j] = np.linalg.pinv(firstTerm) @ secondTerm\n",
    "        \n",
    "        # calculate current joint likelihood\n",
    "        currentLikelihood = CalculateJointLikelihood(in_D=gD, in_R= gR, in_U=U, in_V=V)\n",
    "        \n",
    "        # Show the current value of objective function\n",
    "        print(\"Value => \", currentLikelihood)\n",
    "        \n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSample_UV(in_init_U, in_init_V, in_R, in_D, in_nSamples, in_stepSize):\n",
    "    \n",
    "    U = copy.deepcopy(in_init_U) # init U\n",
    "    V = copy.deepcopy(in_init_V) # init V\n",
    "    \n",
    "    nUsers = len(U)\n",
    "    nMovies = len(V)\n",
    "    c = 1.0\n",
    "    \n",
    "    u_samples = []\n",
    "    v_samples = []\n",
    "    \n",
    "    for n in range(in_nSamples):\n",
    "        for t in range(in_stepSize):\n",
    "            print(\"iteration => \", t)\n",
    "            for i in U.keys():\n",
    "                firstTerm = np.identity(n=kDim,dtype='float') / c\n",
    "                secondTerm = np.zeros([kDim,1],dtype='float')\n",
    "\n",
    "                for j in V.keys():\n",
    "                    if type(in_D.get((i,j))) != type(None):\n",
    "                        firstTerm = firstTerm + (V[j] @ V[j].T)\n",
    "                        secondTerm = secondTerm + (in_R[(i, j)] * V[j])\n",
    "\n",
    "                covTerm = np.linalg.inv(firstTerm)\n",
    "                muTerm = (covTerm @ secondTerm).T\n",
    "\n",
    "                U[i] = np.random.multivariate_normal(muTerm[0], covTerm, 1).T\n",
    "\n",
    "            for j in V.keys():\n",
    "                firstTerm = np.identity(n=kDim,dtype='float') / c\n",
    "                secondTerm = np.zeros([kDim,1],dtype='float')\n",
    "\n",
    "                for i in U.keys():\n",
    "                    if type(in_D.get((i,j))) != type(None):\n",
    "\n",
    "                        firstTerm = firstTerm + (U[i] @ U[i].T)\n",
    "                        secondTerm = secondTerm + (in_R[(i, j)] * U[i])\n",
    "\n",
    "                covTerm = np.linalg.inv(firstTerm)\n",
    "                muTerm = (covTerm @ secondTerm).T\n",
    "\n",
    "                V[j] = np.random.multivariate_normal(muTerm[0], covTerm, 1).T\n",
    "        \n",
    "        print(\"We've got one sample from full posterior distribution\")\n",
    "        u_samples.append(U)\n",
    "        v_samples.append(V)\n",
    "    return u_samples,v_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllErrors_GibbsVersion(in_testFileName, in_D, in_R, in_init_U, in_init_V, in_nSamples):\n",
    "    \n",
    "    USamples, VSamples = GetSample_UV(in_D=in_D,\n",
    "                                      in_R=in_R,\n",
    "                                      in_init_U=in_init_U,\n",
    "                                      in_init_V=in_init_V,\n",
    "                                      in_nSamples=in_nSamples,\n",
    "                                      in_stepSize=10)\n",
    "    \n",
    "    trainingSquaredErrorSum = 0.0\n",
    "    testSquaredErrorSum = 0.0\n",
    "    \n",
    "    ##### training error\n",
    "    for eachKey in in_D.keys():\n",
    "        sum = 0.0\n",
    "        for n in range(in_nSamples):\n",
    "            sum = sum + (USamples[n][eachKey[0]].T @ VSamples[n][eachKey[1]])[0][0]\n",
    "        sum = sum / float(in_nSamples)\n",
    "        trainingSquaredErrorSum = trainingSquaredErrorSum + ((in_R[eachKey] - sum) ** 2)\n",
    "    \n",
    "    trainingSquaredErrorSum = trainingSquaredErrorSum / len(gR)\n",
    "    print('Training Error (Gibbs)', trainingSquaredErrorSum)\n",
    "    \n",
    "    #### test error\n",
    "    test_file = open(in_testFileName, 'r')\n",
    "    fileReader = csv.reader(test_file)\n",
    "    \n",
    "    testSquaredErrorSum = 0.0\n",
    "    testSetCount = 0\n",
    "    for row in fileReader:\n",
    "        if row[0] == 'userId':\n",
    "            continue\n",
    "        else:\n",
    "            testSetCount = testSetCount + 1\n",
    "            currentUserID = int(row[0])\n",
    "            currentMovieID = int(row[1])\n",
    "            currentRating = float(row[2])\n",
    "                \n",
    "            sum = 0.0\n",
    "            for n in range(in_nSamples):\n",
    "                sum = sum + (USamples[n][currentUserID].T @ VSamples[n][currentMovieID])[0][0]\n",
    "            estimatedRating = sum / float(in_nSamples)\n",
    "            \n",
    "            testSquaredErrorSum = testSquaredErrorSum + ((estimatedRating - currentRating) ** 2)\n",
    "    \n",
    "    testSquaredErrorSum = testSquaredErrorSum / float(testSetCount)\n",
    "    print('Test Error (Gibbs)', testSquaredErrorSum)\n",
    "    \n",
    "    return trainingSquaredErrorSum, testSquaredErrorSum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllErrors_MLEVersion(in_testFileName, in_D, in_R, in_init_U, in_init_V, in_nIterations):\n",
    "    \n",
    "    U, V = CalculateMLE(gU, gV, gD, gR, in_nIterations)\n",
    "    trainingSquaredErrorSum = 0.0\n",
    "    testSquaredErrorSum = 0.0\n",
    "    \n",
    "    ##### training error\n",
    "    for eachKey in in_D.keys():\n",
    "        trainingSquaredErrorSum = trainingSquaredErrorSum + (((U[eachKey[0]].T @ V[eachKey[1]])[0][0] - in_R[eachKey]) ** 2)\n",
    "    \n",
    "    trainingSquaredErrorSum = trainingSquaredErrorSum / len(gR)\n",
    "    print('Training Error (MLE)', trainingSquaredErrorSum)\n",
    "    \n",
    "    #### test error\n",
    "    test_file = open(in_testFileName, 'r')\n",
    "    fileReader = csv.reader(test_file)\n",
    "    \n",
    "    testSquaredErrorSum = 0.0\n",
    "    testSetCount = 0\n",
    "    testErrorList = []\n",
    "    \n",
    "    for row in fileReader:\n",
    "        if row[0] == 'userId':\n",
    "            continue\n",
    "        else:\n",
    "            testSetCount = testSetCount + 1\n",
    "            currentUserID = int(row[0])\n",
    "            currentMovieID = int(row[1])\n",
    "            currentRating = float(row[2])\n",
    "                \n",
    "            estimatedRating = (U[currentUserID].T @ V[currentMovieID])[0][0]\n",
    "            testErrorList.append(((estimatedRating - currentRating) ** 2))\n",
    "    \n",
    "    print('Test Error (MLE)', np.mean(testErrorList))\n",
    "    \n",
    "    return trainingSquaredErrorSum, np.mean(testErrorList), testErrorList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gU.clear()\n",
    "gV.clear()\n",
    "\n",
    "# os.remove('wholeTrainingSet.csv')   # fixed    ex) 80% are training\n",
    "# os.remove('testSet.csv')            # fixed    ex) 20% are test\n",
    "\n",
    "# Get all user id and movie id in order to avoid encontering unseen user or movie \n",
    "# init gU, gV\n",
    "GenerateModelVariables (in_file_name='movie_ratings.csv', in_U=gU, in_V=gV)\n",
    "\n",
    "# 20% of whole set is a test set\n",
    "testSetRatio = 0.2\n",
    "MakeTestSet('movie_ratings.csv', testSetRatio, 'wholeTrainingSet.csv', 'testSet.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gD.clear()\n",
    "gR.clear()\n",
    "ChooseSomeOfDataSet(in_file_name='wholeTrainingSet.csv', in_ratio = 0.6, out_trainingFileName='someTrainSet.csv')\n",
    "ReadTrainingCSVFile(in_file_name='someTrainSet.csv',out_D=gD, out_R=gR)\n",
    "GetAllErrors_MLEVersion(in_D=gD, in_R=gR, in_init_U=gU,in_init_V=gV,in_nIterations=30,in_testFileName='testSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingRatios = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "trainingMLE_error_list = []\n",
    "testMLE_error_list = []\n",
    "trainingGibbs_error_list = []\n",
    "testGibbs_error_list = []\n",
    "\n",
    "for i in range(len(trainingRatios)):\n",
    "    gD.clear()\n",
    "    gR.clear()\n",
    "    \n",
    "    print('=====================================')\n",
    "    print('ratio is ==> ', trainingRatios[i], ' ')\n",
    "    print('=====================================')\n",
    "    \n",
    "    # some of trainingSet is also a training set\n",
    "    ChooseSomeOfDataSet(in_file_name='wholeTrainingSet.csv', in_ratio = trainingRatios[i], out_trainingFileName='someTrainSet.csv')\n",
    "    ReadTrainingCSVFile(in_file_name='someTrainSet.csv',out_D=gD, out_R=gR)\n",
    "    GetAllErrors_MLEVersion(in_D=gD, in_R=gR, in_init_U=gU,in_init_V=gV,in_nIterations=30,in_testFileName='testSet.csv')\n",
    "    print(' ')\n",
    "    GetAllErrors_GibbsVersion(in_testFileName='testSet.csv', in_D=gD, in_R=gR, in_init_U=gU, in_init_V=gV, in_nSamples=5)\n",
    "    \n",
    "    os.remove('someTrainSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingRatioIndex = [\"30%\", \"40%\", \"50%\", \"60%\", \"70%\"]\n",
    "MLE_test_errors = [335414, 273182, 126636, 1225665, 143401.4539753]\n",
    "Gibbs_test_errors = [3.8439273060086645, 3.1661662479381523, 2.6981003461349196, 2.5245830148313915,2.2656395976138404]\n",
    "\n",
    "MLE_training_errors = [0.10897821251500237, 0.10423049514512372, 0.12896267601352068, 0.1687827658906558, 0.17746496307287785]\n",
    "Gibbs_training_errors = [0.7276150374115209,  0.69941911662465, 0.6766866324487065, 0.683105207595826, 0.6630517063772686]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(MLE_test_errors,'o--', color='darkgreen', label='MLE Test error')\n",
    "plt.plot(Gibbs_test_errors, 'o--', color='blue', label='Our model Test error')\n",
    "plt.legend()\n",
    "plt.title('Test Errors')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.xticks(range(len(trainingRatios)), trainingRatioIndex)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(MLE_training_errors ,'o--', color='darkgreen', label='MLE Training error')\n",
    "plt.plot(Gibbs_training_errors, 'o--', color='blue', label='Our model Training error')\n",
    "plt.legend()\n",
    "plt.title('Training Errors')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.xticks(range(len(trainingRatios)), trainingRatioIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
