{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ham_address = '/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/ham/'\n",
    "spam_address = '/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/spam/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_file = open('/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/Summary.txt')\n",
    "summary = summary_file.read()\n",
    "summary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hams = []\n",
    "hamfiles = sorted(glob.glob(ham_address+'*.txt'))\n",
    "hfiles = len(hamfiles)\n",
    "for i in range(hfiles):\n",
    "    file = open(hamfiles[i], 'rt')\n",
    "    text = file.read()\n",
    "    hams.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spams = []\n",
    "spamfiles = sorted(glob.glob(spam_address+'*.txt'))\n",
    "sfiles = len(spamfiles)\n",
    "for i in range(sfiles):\n",
    "    file = open(spamfiles[i], 'rt', encoding=\"latin-1\")\n",
    "    text = file.read()\n",
    "    spams.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "ps = PorterStemmer()\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_one(datalist):\n",
    "    \"\"\"\n",
    "    Construct stemmed+bag-of-words model for individual then construct an array of individual bags\n",
    "    \n",
    "    Returns\n",
    "    a collection of individual set dict corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag_collection = []\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = []\n",
    "        token = tokenizer.tokenize(datalist[i])\n",
    "        for w in token:\n",
    "            stemmed.append(ps.stem(w))\n",
    "    \n",
    "        nstem = len(stemmed)\n",
    "        one_bag = {}\n",
    "        \n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in one_bag:\n",
    "                one_bag[key] += 1\n",
    "            else:\n",
    "                one_bag[key] = 1\n",
    "        bag_collection.append(one_bag)\n",
    "        \n",
    "    return bag_collection\n",
    "\n",
    "def embed_whole(datalist):\n",
    "    \"\"\"\n",
    "    From a list of data (should have multiple), do stemming (+remove non-words) then apply the bag-of-words model\n",
    "    \n",
    "    Returns\n",
    "    a dictionary of bag-of-words each dic corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag = {}\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = []\n",
    "        token = tokenizer.tokenize(datalist[i])\n",
    "        for w in token:\n",
    "            stemmed.append(ps.stem(w))\n",
    "        #stemmed = list(set(stemmed))\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in bag:\n",
    "                bag[key] += 1\n",
    "            else:\n",
    "                bag[key] = 1        \n",
    "    return bag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_one_bag = embed_one(hams)\n",
    "spam_one_bag = embed_one(spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = np.concatenate((hams, spams))\n",
    "whole_bag = embed_whole(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_sum_bag = embed_whole(spams)\n",
    "ham_sum_bag = embed_whole(hams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Find the overlapping dictionaries\n",
    "    \"\"\"\n",
    "    key1 = set(dict1.keys())\n",
    "    key2 = set(dict2.keys())\n",
    "    intersection = key1 & key2\n",
    "    newdict1 = {}\n",
    "    newdict2 = {}\n",
    "    intersection = list(intersection)\n",
    "    ninter = len(intersection)\n",
    "    for i in range(ninter):\n",
    "        newdict1[intersection[i]] = dict1[intersection[i]]\n",
    "        newdict2[intersection[i]] = dict2[intersection[i]]\n",
    "    return newdict1, newdict2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovlp_spam, ovlp_ham = overlap(spam_sum_bag, ham_sum_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrase = ['I likes you like you tree']\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "words=tokenizer.tokenize(phrase[0])\n",
    "emp = []\n",
    "for w in words:\n",
    "    emp.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'you', 'like', 'you', 'tree']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorting_hand(dic, sort='descending'):\n",
    "    \"\"\"\n",
    "    Input dictionary and sort it \n",
    "    \"\"\"\n",
    "    if sort == 'descending':\n",
    "        sorted_dic = sorted(dic.items(), key=lambda x: x[1], reverse=True)\n",
    "    elif sort == 'ascending':\n",
    "        sorted_dic = sorted(dic.items(), key=lambda x: x[1], reverse=True)\n",
    "    elif sort == 'diff':\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(train1_count, train2_count):\n",
    "    \"\"\"\n",
    "    Compute Entropy to judge whether this is a good classifier for one dictionary\n",
    "    \"\"\"\n",
    "    total_count = train1_count + train2_count\n",
    "    ratio1 = train1_count / total_count\n",
    "    ratio2 = train2_count / total_count\n",
    "    entropy = - ratio1 * np.log2(ratio1) - ratio2 * np.log2(ratio2)\n",
    "    return entropy\n",
    "    \n",
    "\n",
    "def Decision_Tree(test, train1, train2, entropy_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Compute Entropy of test which overlaps with the training sets with Decision Tree\n",
    "    \n",
    "    Input \n",
    "    test = ['email message']\n",
    "    train1&2 [dict] : a dictionary of total bag-of-words\n",
    "    Returns to entropy\n",
    "    \"\"\"\n",
    "    testing = embed_one(test)[0]\n",
    "    testkeys = testing.keys()\n",
    "    train1_keys = train1.keys()\n",
    "    train2_keys = train2.keys()\n",
    "    merged_keys = (set(train1_keys) | set(train2_keys))\n",
    "    #testkeys_list = list(testkeys)\n",
    "    ntest = len(testkeys)\n",
    "    tk1_only = list(set(train1_keys)-set(train2_keys))\n",
    "    tk2_only = list(set(train2_keys)-set(train1_keys))\n",
    "    all_train = np.vstack((train1, train2))\n",
    "    \n",
    "    weights = np.ones(2)\n",
    "\n",
    "    for tk in testkeys:\n",
    "        if tk in tk1_only:\n",
    "            weights[0] *= 1000\n",
    "        elif tk in tk2_only:\n",
    "            weights[1] *= 1000\n",
    "        elif tk not in merged_keys:\n",
    "            pass\n",
    "        else:\n",
    "            entropy = compute_entropy(train1[tk], train2[tk])\n",
    "            if entropy >= entropy_thresh:\n",
    "                #print (testing[tk], train1[tk], train2[tk])\n",
    "                weights[0] *= testing[tk] / train1[tk]\n",
    "                weights[1] *= testing[tk] / train2[tk]  \n",
    "                \n",
    "    return weights \n",
    "    \n",
    "\n",
    "def Naive_Bayes(test, train, prior=1500/5172):\n",
    "    \"\"\"\n",
    "    Compute the score of test with respect to training sets using Naive Bayes\n",
    "    \n",
    "    Input\n",
    "    test = ['email message']\n",
    "    train [dict] : a dictionary of total bag-of-words\n",
    "    prior = N_email / N_total (eg. 1500/5172 for SPAM or 3672/1500 for HAM)\n",
    "    Returns to scores\n",
    "    \"\"\"\n",
    "    train_keys = train.keys()\n",
    "    all_train_vals = sum(list(train.values())) ## sum of all occurances\n",
    "    \n",
    "    ### find the overlapping term\n",
    "    testing = embed_one(test)[0]\n",
    "    testkeys = testing.keys()\n",
    "    ntest = len(testkeys)\n",
    "    \n",
    "    score = prior\n",
    "    \n",
    "    for i in range(ntest):\n",
    "        denominator = (all_train_vals + ntest+1)\n",
    "        tkey = list(testkeys)[i]\n",
    "        if tkey in train_keys:\n",
    "            multi_factor = testing[tkey]\n",
    "            score *= ((train[tkey] + 1) / denominator)**(multi_factor)\n",
    "        else:\n",
    "            score *= 1 / denominator\n",
    "            \n",
    "    return score\n",
    "\n",
    "\n",
    "def NN_Distances(test, train, nn_option='L2'):\n",
    "    \"\"\"\n",
    "    Compute the distance of test based on training sets using Nearest Neighbor\n",
    "    test: test [one_email] ; train (individual dictionaries)\n",
    "    \n",
    "    Input:\n",
    "    test = ['email message']\n",
    "    train [list] : a collection of individual set dict\n",
    "    \n",
    "    Returns to the array of distances\n",
    "    \"\"\"\n",
    "    testing = embed_one(test)[0] ## because it's just one\n",
    "    testkeys = testing.keys()\n",
    "    ntest = len(testkeys)\n",
    "\n",
    "    ntrain = len(train)\n",
    "    dist = np.zeros(ntrain)\n",
    "    \n",
    "    for i in range(ntrain):\n",
    "        onekey = train[i].keys()\n",
    "        one_eval = testing.copy()\n",
    "        one_eval.update(train[i]) ## for all un-matching dictionaries\n",
    "        for j in range(ntest):\n",
    "            if list(testkeys)[j] in onekey:\n",
    "                thiskey = list(testkeys)[j]\n",
    "                one_eval[thiskey] = train[i][thiskey] - testing[thiskey] ## subtract only when items are matching\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        one_eval = np.array(list(one_eval.values()))\n",
    "        #print (one_eval)\n",
    "        if nn_option == 'L1' or nn_option == 'Linf':\n",
    "            dist[i] = sum(np.abs(one_eval))\n",
    "        elif nn_option == 'L2':\n",
    "            dist[i] = np.sqrt(sum(one_eval**2))\n",
    "            \n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16.58312395,  10.95445115,   6.92820323, ...,   5.74456265,\n",
       "         5.74456265,  16.37070554])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Distances(phrase, spam_one_bag, nn_option='L2') ## Returns to distance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8051631976513072e-22"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Naive_Bayes(phrase, ham_sum_bag) ## Returns to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.35642591e-07,   4.74802019e-07])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decision_Tree(phrase, ham_sum_bag, spam_sum_bag, entropy_thresh=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: meter 1431 - nov 1999\\ndaren -\\ncould you please resolve this issue for howard ? i will be out of the office\\nthe next two days .\\nwhen this is done , please let george know . thanks .\\naimee\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by aimee lannou / hou / ect on 12 / 15 / 99 01 : 27 pm\\n- - - - - - - - - - - - - - - - - - - - - - - - - - -\\nhoward b camp\\n12 / 15 / 99 01 : 01 pm\\nto : aimee lannou / hou / ect @ ect\\ncc : daren j farmer / hou / ect @ ect , stacey neuweiler / hou / ect @ ect , mary m\\nsmith / hou / ect @ ect\\nsubject : meter 1431 - nov 1999\\naimee ,\\nsitara deal 92943 for meter 1431 has expired on oct 31 , 1999 . settlements\\nis unable to draft an invoice for this deal . this deal either needs to be\\nextended or a new deal needs to be set up . please let me know when this is\\nresolved . we need it resolved by friday , dec 17 .\\nhc'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_testing[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-3f2e2a2137e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_Distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_testing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mham_sum_bag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-134-d5098d415060>\u001b[0m in \u001b[0;36mNN_Distances\u001b[0;34m(test, train, nn_option)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0monekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mone_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mone_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## for all un-matching dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "NN_Distances(ham_testing[10], ham_sum_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns to a common measure\n",
    "def Classifier(new, one, two, option='NB', nn_option='L2'):\n",
    "    \"\"\"\n",
    "    For \"overlapping\" bag of words between new and training sets, evaluate probability based on classifier of choice\n",
    "    NB: Naive Bayes; DT: Decision Tree; NN: Nearest Neighbors\n",
    "    New (to-be-examined); one(training 1); two(training 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    ntest = len(new)\n",
    "    scores = np.zeros((ntest,2))\n",
    "    if option == 'NN':\n",
    "        one_bag = embed_one(one) \n",
    "        two_bag = embed_one(two)\n",
    "    else:\n",
    "        one_bag = embed_whole(one)\n",
    "        two_bag = embed_whole(two)\n",
    "        \n",
    "    for n in range(ntest):\n",
    "        if option == 'NN':\n",
    "            dist_one = NN_Distances(new[n], one_bag, nn_option)\n",
    "            dist_two = NN_Distances(new[n], two_bag, nn_option)\n",
    "            if nn_option == 'L1' or nn_option == 'L2':\n",
    "                scores[n,0] = 1 / np.sum(dist_one) ## only for NN smaller number signifies better score\n",
    "                scores[n,1] = 1 / np.sum(dist_two)\n",
    "            elif nn_option == 'Linf':\n",
    "                scores[n,0] = np.max(dist_one)\n",
    "                scores[n,1] = np.max(dist_two)\n",
    "\n",
    "        elif option == 'NB':\n",
    "            scores[n,0] = Naive_Bayes(new[n], one_bag)\n",
    "            scores[n,1] = Naive_Bayes(new[n], two_bag)\n",
    "\n",
    "        elif option == 'DT':\n",
    "            scores[n] = Decision_Tree(new[n], one_bag, two_bag, entropy_thresh=0.8)\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((8,2))[7,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TESTING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Doyeon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: This function is deprecated. Please call randint(0, 3500 + 1) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/Doyeon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: This function is deprecated. Please call randint(0, 1499 + 1) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "random_draw0 = np.random.random_integers(0,3500,1000)\n",
    "ramdom_draw1 = np.random.random_integers(0,1499,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hams_array = np.array(hams.copy())\n",
    "spams_array = np.array(spams.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test_index = np.array(list(set(np.arange(len(hams)))- set(random_draw0))[:100])\n",
    "spam_test_index = np.array(list(set(np.arange(len(spams)))- set(ramdom_draw1))[:100])\n",
    "\n",
    "ham_testing = hams_array[[ham_test_index]]\n",
    "spam_testing = spams_array[[spam_test_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_training = hams_array[[random_draw0]]\n",
    "spam_training = spams_array[[ramdom_draw1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NB', nn_option='L2')\n",
    "nb_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NB', nn_option='L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00019363  0.00016005] [ 0.00019363  0.00016005]\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(nb_scores_ham, axis=0), np.sum(nb_scores_spam, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnl1_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NN', nn_option='L1')\n",
    "nnl1_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NN', nn_option='L1')\n",
    "\n",
    "nnl2_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NN', nn_option='L2')\n",
    "nnl2_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NN', nn_option='L2')\n",
    "\n",
    "nnlinf_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NN', nn_option='Linf')\n",
    "nnlinf_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NN', nn_option='Linf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0006632   0.00054884] [ 0.0006632   0.00054884]\n",
      "[ 0.00489144  0.00479541] [ 0.00489144  0.00479541]\n",
      "[ 275800.  233400.] [ 275800.  233400.]\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(nnl1_scores_ham, axis=0), np.sum(nnl1_scores_spam, axis=0))\n",
    "print (np.sum(nnl2_scores_ham, axis=0), np.sum(nnl2_scores_spam, axis=0))\n",
    "print (np.sum(nnlinf_scores_ham, axis=0), np.sum(nnlinf_scores_spam, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### only evaluate the overlapping keywords?\n",
    "inter_spam, inter_ham = overlap(spam_bag, ham_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INCORRECT (Without Stemming)\n",
    "def embed_whole(datalist):\n",
    "    \"\"\"\n",
    "    From a list of data (should have multiple), do stemming (+remove non-words) then apply the bag-of-words model\n",
    "    \n",
    "    Returns\n",
    "    a dictionary of bag-of-words each dic corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag = {}\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = tokenizer.tokenize(datalist[i])\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in bag:\n",
    "                bag[key] += 1\n",
    "            else:\n",
    "                bag[key] = 1        \n",
    "    return bag\n",
    "\n",
    "def embed_one(datalist):\n",
    "    \"\"\"\n",
    "    Construct stemmed+bag-of-words model for individual then construct an array of individual bags\n",
    "    \n",
    "    Returns\n",
    "    a collection of individual set dict corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag_collection = []\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        one_bag = {}\n",
    "        stemmed = tokenizer.tokenize(datalist[i])\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in one_bag:\n",
    "                one_bag[key] += 1\n",
    "            else:\n",
    "                one_bag[key] = 1\n",
    "        bag_collection.append(one_bag)\n",
    "        \n",
    "    return bag_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
