{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ham_address = '/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/ham/'\n",
    "spam_address = '/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/spam/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_file = open('/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/Summary.txt')\n",
    "summary = summary_file.read()\n",
    "summary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hams = []\n",
    "hamfiles = sorted(glob.glob(ham_address+'*.txt'))\n",
    "hfiles = len(hamfiles)\n",
    "for i in range(hfiles):\n",
    "    file = open(hamfiles[i], 'rt')\n",
    "    text = file.read()\n",
    "    hams.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spams = []\n",
    "spamfiles = sorted(glob.glob(spam_address+'*.txt'))\n",
    "sfiles = len(spamfiles)\n",
    "for i in range(sfiles):\n",
    "    file = open(spamfiles[i], 'rt', encoding=\"latin-1\")\n",
    "    text = file.read()\n",
    "    spams.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "ps = PorterStemmer()\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_one(datalist):\n",
    "    \"\"\"\n",
    "    Construct stemmed+bag-of-words model for individual then construct an array of individual bags\n",
    "    \n",
    "    Returns\n",
    "    a collection of individual set dict corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag_collection = []\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = []\n",
    "        token = tokenizer.tokenize(datalist[i])\n",
    "        for w in token:\n",
    "            stemmed.append(ps.stem(w))\n",
    "    \n",
    "        nstem = len(stemmed)\n",
    "        one_bag = {}\n",
    "        \n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in one_bag:\n",
    "                one_bag[key] += 1\n",
    "            else:\n",
    "                one_bag[key] = 1\n",
    "        bag_collection.append(one_bag)\n",
    "        \n",
    "    return bag_collection\n",
    "\n",
    "def embed_whole(datalist):\n",
    "    \"\"\"\n",
    "    From a list of data (should have multiple), do stemming (+remove non-words) then apply the bag-of-words model\n",
    "    \n",
    "    Returns\n",
    "    a dictionary of bag-of-words each dic corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag = {}\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = []\n",
    "        token = tokenizer.tokenize(datalist[i])\n",
    "        for w in token:\n",
    "            stemmed.append(ps.stem(w))\n",
    "        #stemmed = list(set(stemmed))\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in bag:\n",
    "                bag[key] += 1\n",
    "            else:\n",
    "                bag[key] = 1        \n",
    "    return bag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_one_bag = embed_one(hams)\n",
    "spam_one_bag = embed_one(spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = np.concatenate((hams, spams))\n",
    "whole_bag = embed_whole(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_sum_bag = embed_whole(spams)\n",
    "ham_sum_bag = embed_whole(hams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Find the overlapping dictionaries\n",
    "    \"\"\"\n",
    "    key1 = set(dict1.keys())\n",
    "    key2 = set(dict2.keys())\n",
    "    intersection = key1 & key2\n",
    "    newdict1 = {}\n",
    "    newdict2 = {}\n",
    "    intersection = list(intersection)\n",
    "    ninter = len(intersection)\n",
    "    for i in range(ninter):\n",
    "        newdict1[intersection[i]] = dict1[intersection[i]]\n",
    "        newdict2[intersection[i]] = dict2[intersection[i]]\n",
    "    return newdict1, newdict2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovlp_spam, ovlp_ham = overlap(spam_sum_bag, ham_sum_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrase = ['I likes you like you tree']\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "words=tokenizer.tokenize(phrase[0])\n",
    "emp = []\n",
    "for w in words:\n",
    "    emp.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'you', 'like', 'you', 'tree']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_dicts = key_differences(spam_sum_bag, ham_sum_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_differences(dict1, dict2):\n",
    "    dict1_key = dict1.keys()\n",
    "    dict2_key = dict2.keys()\n",
    "    dict1_only = list(set(dict1_key)-set(dict2_key))\n",
    "    dict2_only = list(set(dict2_key)-set(dict1_key))\n",
    "    \n",
    "    allkeys = list(dict1_key|dict2_key)\n",
    "    \n",
    "    diff_dics = {}\n",
    "    for i in range(len(allkeys)):\n",
    "        if allkeys[i] in dict1_only:\n",
    "            diff_dics[allkeys[i]] = dict1[allkeys[i]]\n",
    "        elif allkeys[i] in dict2_only:\n",
    "            diff_dics[allkeys[i]] = dict2[allkeys[i]]\n",
    "        else:\n",
    "            diff_dics[allkeys[i]] = np.abs(dict1[allkeys[i]] -  dict2[allkeys[i]])\n",
    "    \n",
    "    diff_dics = sorting_hand(diff_dics)\n",
    "    return diff_dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorting_hand(dic, sort='descending'):\n",
    "    \"\"\"\n",
    "    Input dictionary and sort it \n",
    "    \"\"\"\n",
    "    if sort == 'descending':\n",
    "        sorted_dic = sorted(dic.items(), key=lambda x: x[1], reverse=True)\n",
    "    elif sort == 'ascending':\n",
    "        sorted_dic = sorted(dic.items(), key=lambda x: x[1], reverse=False)\n",
    "    return sorted_dic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_whole = sorting_hand(whole_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ect', 13886),\n",
       " ('the', 11061),\n",
       " ('to', 9999),\n",
       " ('hou', 7273),\n",
       " ('enron', 6555),\n",
       " ('for', 5456),\n",
       " ('on', 4782),\n",
       " ('subject', 4745),\n",
       " ('2000', 4230),\n",
       " ('i', 4091),\n",
       " ('deal', 3443),\n",
       " ('and', 2994),\n",
       " ('will', 2741),\n",
       " ('ga', 2718),\n",
       " ('meter', 2715),\n",
       " ('is', 2642),\n",
       " ('be', 2602),\n",
       " ('thi', 2583),\n",
       " ('you', 2568),\n",
       " ('have', 2383),\n",
       " ('cc', 2367),\n",
       " ('hpl', 2318),\n",
       " ('pm', 2307),\n",
       " ('am', 2277),\n",
       " ('at', 2246),\n",
       " ('a', 2230),\n",
       " ('pleas', 2229),\n",
       " ('from', 2228),\n",
       " ('we', 2071),\n",
       " ('that', 2066),\n",
       " ('if', 2037),\n",
       " ('2001', 2020),\n",
       " ('thank', 1945),\n",
       " ('daren', 1901),\n",
       " ('com', 1724),\n",
       " ('by', 1658),\n",
       " ('01', 1652),\n",
       " ('volum', 1645),\n",
       " ('corp', 1644),\n",
       " ('000', 1633),\n",
       " ('me', 1546),\n",
       " ('re', 1529),\n",
       " ('10', 1517),\n",
       " ('in', 1427),\n",
       " ('mmbtu', 1408),\n",
       " ('forward', 1383),\n",
       " ('need', 1379),\n",
       " ('1', 1303),\n",
       " ('know', 1298),\n",
       " ('j', 1264),\n",
       " ('00', 1234),\n",
       " ('nom', 1184),\n",
       " ('11', 1182),\n",
       " ('d', 1182),\n",
       " ('of', 1170),\n",
       " ('03', 1148),\n",
       " ('attach', 1143),\n",
       " ('chang', 1130),\n",
       " ('farmer', 1122),\n",
       " ('contract', 1075),\n",
       " ('12', 1067),\n",
       " ('are', 1052),\n",
       " ('with', 1045),\n",
       " ('day', 1035),\n",
       " ('xl', 1021),\n",
       " ('let', 1021),\n",
       " ('nomin', 997),\n",
       " ('it', 994),\n",
       " ('02', 988),\n",
       " ('wa', 948),\n",
       " ('should', 940),\n",
       " ('flow', 918),\n",
       " ('ani', 887),\n",
       " ('robert', 878),\n",
       " ('juli', 873),\n",
       " ('not', 868),\n",
       " ('04', 863),\n",
       " ('sitara', 861),\n",
       " ('see', 838),\n",
       " ('question', 829),\n",
       " ('05', 810),\n",
       " ('ticket', 808),\n",
       " ('can', 796),\n",
       " ('s', 787),\n",
       " ('month', 776),\n",
       " ('08', 768),\n",
       " ('texa', 765),\n",
       " ('09', 762),\n",
       " ('as', 755),\n",
       " ('pec', 752),\n",
       " ('l', 738),\n",
       " ('follow', 735),\n",
       " ('ena', 732),\n",
       " ('there', 728),\n",
       " ('http', 726),\n",
       " ('bob', 702),\n",
       " ('e', 700),\n",
       " ('would', 672),\n",
       " ('2', 663),\n",
       " ('ha', 663),\n",
       " ('file', 660),\n",
       " ('call', 654),\n",
       " ('do', 639),\n",
       " ('up', 627),\n",
       " ('06', 621),\n",
       " ('they', 614),\n",
       " ('30', 597),\n",
       " ('07', 589),\n",
       " ('4', 589),\n",
       " ('schedul', 584),\n",
       " ('been', 579),\n",
       " ('_', 573),\n",
       " ('so', 572),\n",
       " ('new', 569),\n",
       " ('actual', 566),\n",
       " ('alloc', 563),\n",
       " ('purchas', 562),\n",
       " ('gari', 558),\n",
       " ('713', 556),\n",
       " ('31', 546),\n",
       " ('5', 545),\n",
       " ('mari', 541),\n",
       " ('daili', 540),\n",
       " ('origin', 539),\n",
       " ('into', 539),\n",
       " ('may', 538),\n",
       " ('sale', 537),\n",
       " ('these', 526),\n",
       " ('energi', 521),\n",
       " ('font', 513),\n",
       " ('th', 506),\n",
       " ('what', 502),\n",
       " ('td', 500),\n",
       " ('sent', 499),\n",
       " ('april', 496),\n",
       " ('transport', 490),\n",
       " ('march', 481),\n",
       " ('out', 479),\n",
       " ('product', 477),\n",
       " ('manag', 476),\n",
       " ('plant', 475),\n",
       " ('or', 474),\n",
       " ('20', 472),\n",
       " ('but', 470),\n",
       " ('melissa', 462),\n",
       " ('m', 458),\n",
       " ('deliveri', 449),\n",
       " ('3', 444),\n",
       " ('teco', 444),\n",
       " ('revis', 443),\n",
       " ('houston', 442),\n",
       " ('vanc', 442),\n",
       " ('messag', 435),\n",
       " ('21', 433),\n",
       " ('fuel', 432),\n",
       " ('tenaska', 431),\n",
       " ('16', 430),\n",
       " ('6', 429),\n",
       " ('issu', 428),\n",
       " ('99', 426),\n",
       " ('invest', 424),\n",
       " ('28', 423),\n",
       " ('ami', 422),\n",
       " ('nbsp', 418),\n",
       " ('www', 418),\n",
       " ('secur', 416),\n",
       " ('statement', 410),\n",
       " ('compani', 410),\n",
       " ('request', 409),\n",
       " ('pat', 398),\n",
       " ('georg', 395),\n",
       " ('mail', 393),\n",
       " ('get', 392),\n",
       " ('path', 389),\n",
       " ('fw', 387),\n",
       " ('stock', 387),\n",
       " ('t', 385),\n",
       " ('june', 385),\n",
       " ('effect', 381),\n",
       " ('smith', 381),\n",
       " ('9', 379),\n",
       " ('when', 376),\n",
       " ('pipelin', 374),\n",
       " ('jacki', 373),\n",
       " ('some', 373),\n",
       " ('26', 367),\n",
       " ('list', 363),\n",
       " ('height', 361),\n",
       " ('pill', 357),\n",
       " ('activ', 357),\n",
       " ('aime', 357),\n",
       " ('22', 356),\n",
       " ('creat', 356),\n",
       " ('desk', 353),\n",
       " ('23', 353),\n",
       " ('price', 349),\n",
       " ('13', 347),\n",
       " ('lisa', 347),\n",
       " ('29', 345),\n",
       " ('set', 344),\n",
       " ('14', 342),\n",
       " ('18', 341),\n",
       " ('tap', 341),\n",
       " ('also', 340),\n",
       " ('agreement', 337),\n",
       " ('point', 335),\n",
       " ('susan', 334),\n",
       " ('per', 332),\n",
       " ('hsc', 330),\n",
       " ('meet', 329),\n",
       " ('25', 328),\n",
       " ('24', 328),\n",
       " ('david', 327),\n",
       " ('15', 327),\n",
       " ('octob', 324),\n",
       " ('help', 321),\n",
       " ('cotten', 321),\n",
       " ('show', 320),\n",
       " ('number', 320),\n",
       " ('19', 316),\n",
       " ('27', 315),\n",
       " ('michael', 315),\n",
       " ('mark', 314),\n",
       " ('want', 314),\n",
       " ('counterparti', 313),\n",
       " ('size', 312),\n",
       " ('megan', 308),\n",
       " ('north', 308),\n",
       " ('chokshi', 308),\n",
       " ('taylor', 307),\n",
       " ('steve', 307),\n",
       " ('below', 306),\n",
       " ('invoic', 306),\n",
       " ('width', 305),\n",
       " ('august', 304),\n",
       " ('17', 303),\n",
       " ('addit', 302),\n",
       " ('2004', 299),\n",
       " ('fyi', 298),\n",
       " ('iv', 298),\n",
       " ('well', 297),\n",
       " ('were', 296),\n",
       " ('an', 295),\n",
       " ('januari', 293),\n",
       " ('hplc', 292),\n",
       " ('duke', 291),\n",
       " ('think', 290),\n",
       " ('them', 290),\n",
       " ('back', 290),\n",
       " ('tom', 287),\n",
       " ('wellhead', 287),\n",
       " ('america', 286),\n",
       " ('john', 285),\n",
       " ('tu', 283),\n",
       " ('correct', 279),\n",
       " ('util', 278),\n",
       " ('current', 277),\n",
       " ('clyne', 276),\n",
       " ('friday', 276),\n",
       " ('work', 276),\n",
       " ('eastran', 273),\n",
       " ('time', 271),\n",
       " ('novemb', 269),\n",
       " ('updat', 268),\n",
       " ('receiv', 268),\n",
       " ('lloyd', 268),\n",
       " ('septemb', 268),\n",
       " ('date', 267),\n",
       " ('februari', 266),\n",
       " ('more', 264),\n",
       " ('decemb', 264),\n",
       " ('grave', 261),\n",
       " ('unifi', 261),\n",
       " ('he', 261),\n",
       " ('spot', 260),\n",
       " ('which', 259),\n",
       " ('confirm', 255),\n",
       " ('enter', 254),\n",
       " ('brian', 253),\n",
       " ('8', 252),\n",
       " ('feb', 252),\n",
       " ('x', 247),\n",
       " ('aol', 246),\n",
       " ('team', 245),\n",
       " ('lee', 244),\n",
       " ('each', 243),\n",
       " ('computron', 242),\n",
       " ('about', 242),\n",
       " ('fee', 241),\n",
       " ('na', 241),\n",
       " ('here', 241),\n",
       " ('their', 240),\n",
       " ('co', 240),\n",
       " ('employe', 239),\n",
       " ('group', 239),\n",
       " ('pg', 237),\n",
       " ('money', 236),\n",
       " ('resourc', 236),\n",
       " ('week', 235),\n",
       " ('txu', 235),\n",
       " ('sure', 229),\n",
       " ('meyer', 229),\n",
       " ('just', 228),\n",
       " ('system', 227),\n",
       " ('monday', 227),\n",
       " ('note', 226),\n",
       " ('net', 225),\n",
       " ('had', 225),\n",
       " ('hplno', 224),\n",
       " ('tr', 224),\n",
       " ('howard', 223),\n",
       " ('u', 223),\n",
       " ('rita', 222),\n",
       " ('lannou', 221),\n",
       " ('did', 220),\n",
       " ('align', 220),\n",
       " ('total', 218),\n",
       " ('hank', 217),\n",
       " ('plan', 217),\n",
       " ('suppli', 215),\n",
       " ('tx', 215),\n",
       " ('w', 212),\n",
       " ('rate', 210),\n",
       " ('remov', 210),\n",
       " ('servic', 208),\n",
       " ('extend', 207),\n",
       " ('problem', 206),\n",
       " ('thursday', 206),\n",
       " ('discuss', 205),\n",
       " ('could', 203),\n",
       " ('until', 203),\n",
       " ('jame', 203),\n",
       " ('first', 203),\n",
       " ('don', 203),\n",
       " ('color', 202),\n",
       " ('98', 202),\n",
       " ('she', 202),\n",
       " ('jan', 201),\n",
       " ('like', 201),\n",
       " ('process', 200),\n",
       " ('take', 199),\n",
       " ('buyback', 199),\n",
       " ('under', 198),\n",
       " ('pef', 198),\n",
       " ('trade', 197),\n",
       " ('enronxg', 197),\n",
       " ('weissman', 196),\n",
       " ('transact', 196),\n",
       " ('spreadsheet', 196),\n",
       " ('end', 196),\n",
       " ('cec', 195),\n",
       " ('make', 195),\n",
       " ('oper', 195),\n",
       " ('within', 195),\n",
       " ('who', 195),\n",
       " ('thru', 194),\n",
       " ('gc', 194),\n",
       " ('katherin', 193),\n",
       " ('mike', 193),\n",
       " ('wynn', 192),\n",
       " ('border', 192),\n",
       " ('name', 191),\n",
       " ('href', 190),\n",
       " ('between', 190),\n",
       " ('hplo', 190),\n",
       " ('offer', 190),\n",
       " ('0', 190),\n",
       " ('doc', 189),\n",
       " ('carlo', 189),\n",
       " ('less', 189),\n",
       " ('camp', 188),\n",
       " ('microsoft', 188),\n",
       " ('demand', 188),\n",
       " ('respons', 188),\n",
       " ('global', 187),\n",
       " ('pipe', 187),\n",
       " ('69', 186),\n",
       " ('go', 186),\n",
       " ('iferc', 185),\n",
       " ('handl', 185),\n",
       " ('viagra', 184),\n",
       " ('v', 183),\n",
       " ('inform', 183),\n",
       " ('news', 183),\n",
       " ('zero', 182),\n",
       " ('tuesday', 182),\n",
       " ('allen', 181),\n",
       " ('my', 181),\n",
       " ('use', 180),\n",
       " ('logist', 180),\n",
       " ('face', 180),\n",
       " ('million', 179),\n",
       " ('period', 179),\n",
       " ('howev', 178),\n",
       " ('7', 178),\n",
       " ('one', 178),\n",
       " ('put', 178),\n",
       " ('anita', 177),\n",
       " ('done', 177),\n",
       " ('br', 176),\n",
       " ('imbal', 176),\n",
       " ('softwar', 176),\n",
       " ('act', 175),\n",
       " ('still', 175),\n",
       " ('k', 174),\n",
       " ('equistar', 174),\n",
       " ('profession', 173),\n",
       " ('ask', 173),\n",
       " ('window', 172),\n",
       " ('reflect', 171),\n",
       " ('william', 171),\n",
       " ('sherlyn', 170),\n",
       " ('agre', 170),\n",
       " ('dec', 169),\n",
       " ('fred', 169),\n",
       " ('young', 169),\n",
       " ('adjust', 169),\n",
       " ('gather', 168),\n",
       " ('swing', 168),\n",
       " ('begin', 168),\n",
       " ('next', 168),\n",
       " ('save', 168),\n",
       " ('xp', 168),\n",
       " ('kati', 168),\n",
       " ('prescript', 167),\n",
       " ('ls', 166),\n",
       " ('entex', 166),\n",
       " ('survey', 165),\n",
       " ('adob', 164),\n",
       " ('medic', 163),\n",
       " ('down', 163),\n",
       " ('jone', 163),\n",
       " ('src', 163),\n",
       " ('through', 163),\n",
       " ('onlin', 163),\n",
       " ('report', 162),\n",
       " ('1999', 162),\n",
       " ('clem', 161),\n",
       " ('both', 161),\n",
       " ('result', 161),\n",
       " ('check', 160),\n",
       " ('darren', 159),\n",
       " ('redeliveri', 159),\n",
       " ('wednesday', 158),\n",
       " ('book', 158),\n",
       " ('tess', 158),\n",
       " ('coastal', 157),\n",
       " ('possibl', 157),\n",
       " ('calpin', 155),\n",
       " ('review', 154),\n",
       " ('except', 154),\n",
       " ('firm', 154),\n",
       " ('pop', 154),\n",
       " ('ciali', 154),\n",
       " ('start', 153),\n",
       " ('brenda', 152),\n",
       " ('all', 152),\n",
       " ('valley', 151),\n",
       " ('assign', 150),\n",
       " ('60', 150),\n",
       " ('gco', 150),\n",
       " ('aep', 150),\n",
       " ('midcon', 150),\n",
       " ('ll', 150),\n",
       " ('ee', 149),\n",
       " ('ray', 149),\n",
       " ('scott', 148),\n",
       " ('valid', 148),\n",
       " ('last', 147),\n",
       " ('dure', 147),\n",
       " ('send', 147),\n",
       " ('stephani', 146),\n",
       " ('donna', 146),\n",
       " ('field', 146),\n",
       " ('ranch', 146),\n",
       " ('tomorrow', 145),\n",
       " ('share', 145),\n",
       " ('stacey', 145),\n",
       " ('drug', 145),\n",
       " ('technolog', 145),\n",
       " ('contact', 145),\n",
       " ('anyth', 145),\n",
       " ('estim', 144),\n",
       " ('fax', 144),\n",
       " ('order', 144),\n",
       " ('karen', 144),\n",
       " ('oil', 143),\n",
       " ('thu', 143),\n",
       " ('lauri', 143),\n",
       " ('outag', 143),\n",
       " ('500', 143),\n",
       " ('acton', 142),\n",
       " ('give', 142),\n",
       " ('cotton', 142),\n",
       " ('sinc', 142),\n",
       " ('rodriguez', 141),\n",
       " ('med', 141),\n",
       " ('63', 141),\n",
       " ('donald', 141),\n",
       " ('abl', 141),\n",
       " ('soft', 140),\n",
       " ('due', 140),\n",
       " ('after', 140),\n",
       " ('onc', 139),\n",
       " ('then', 138),\n",
       " ('best', 138),\n",
       " ('parker', 138),\n",
       " ('world', 137),\n",
       " ('two', 137),\n",
       " ('sept', 137),\n",
       " ('161', 137),\n",
       " ('cornhusk', 137),\n",
       " ('again', 137),\n",
       " ('base', 137),\n",
       " ('data', 137),\n",
       " ('95', 136),\n",
       " ('how', 136),\n",
       " ('cover', 135),\n",
       " ('vlt', 135),\n",
       " ('advic', 134),\n",
       " ('intern', 134),\n",
       " ('beaumont', 134),\n",
       " ('jim', 134),\n",
       " ('victor', 133),\n",
       " ('place', 133),\n",
       " ('same', 133),\n",
       " ('im', 132),\n",
       " ('item', 132),\n",
       " ('cynthia', 132),\n",
       " ('continu', 131),\n",
       " ('email', 131),\n",
       " ('schumack', 131),\n",
       " ('90', 131),\n",
       " ('richard', 131),\n",
       " ('cd', 130),\n",
       " ('no', 130),\n",
       " ('carthag', 129),\n",
       " ('track', 129),\n",
       " ('support', 129),\n",
       " ('hotmail', 129),\n",
       " ('come', 129),\n",
       " ('expir', 128),\n",
       " ('2003', 128),\n",
       " ('futur', 128),\n",
       " ('feedback', 128),\n",
       " ('reinhardt', 128),\n",
       " ('either', 127),\n",
       " ('luong', 127),\n",
       " ('someon', 127),\n",
       " ('40', 127),\n",
       " ('dave', 126),\n",
       " ('locat', 125),\n",
       " ('ken', 125),\n",
       " ('hess', 124),\n",
       " ('800', 124),\n",
       " ('abov', 124),\n",
       " ('bill', 124),\n",
       " ('charli', 124),\n",
       " ('id', 124),\n",
       " ('shut', 123),\n",
       " ('newslett', 123),\n",
       " ('regard', 123),\n",
       " ('lsk', 123),\n",
       " ('keep', 123),\n",
       " ('cheryl', 122),\n",
       " ('settlement', 122),\n",
       " ('hplnl', 122),\n",
       " ('herod', 122),\n",
       " ('facil', 121),\n",
       " ('methanol', 121),\n",
       " ('produc', 120),\n",
       " ('papayoti', 119),\n",
       " ('281', 119),\n",
       " ('6353', 119),\n",
       " ('style', 119),\n",
       " ('talk', 118),\n",
       " ('stone', 118),\n",
       " ('paliourg', 118),\n",
       " ('albrecht', 118),\n",
       " ('access', 118),\n",
       " ('resolv', 118),\n",
       " ('dfarmer', 117),\n",
       " ('edward', 117),\n",
       " ('term', 117),\n",
       " ('bryan', 117),\n",
       " ('jeff', 116),\n",
       " ('custom', 116),\n",
       " ('amount', 116),\n",
       " ('lamphier', 116),\n",
       " ('your', 116),\n",
       " ('where', 116),\n",
       " ('ic', 116),\n",
       " ('soon', 115),\n",
       " ('valero', 115),\n",
       " ('80', 115),\n",
       " ('option', 114),\n",
       " ('pro', 114),\n",
       " ('free', 113),\n",
       " ('move', 113),\n",
       " ('cleburn', 112),\n",
       " ('hope', 112),\n",
       " ('exxon', 112),\n",
       " ('clear', 112),\n",
       " ('voip', 112),\n",
       " ('final', 112),\n",
       " ('baumbach', 111),\n",
       " ('materi', 111),\n",
       " ('mani', 111),\n",
       " ('natur', 111),\n",
       " ('poorman', 110),\n",
       " ('nd', 110),\n",
       " ('morn', 110),\n",
       " ('fact', 109),\n",
       " ('roll', 109),\n",
       " ('low', 108),\n",
       " ('complet', 108),\n",
       " ('neal', 108),\n",
       " ('thoma', 108),\n",
       " ('liz', 108),\n",
       " ('special', 108),\n",
       " ('html', 107),\n",
       " ('pt', 107),\n",
       " ('devon', 107),\n",
       " ('anoth', 107),\n",
       " ('oo', 107),\n",
       " ('php', 106),\n",
       " ('afternoon', 106),\n",
       " ('eol', 106),\n",
       " ('sourc', 106),\n",
       " ('union', 106),\n",
       " ('ce', 105),\n",
       " ('bgcolor', 105),\n",
       " ('koch', 105),\n",
       " ('jennif', 105),\n",
       " ('lsp', 105),\n",
       " ('waha', 105),\n",
       " ('eb', 104),\n",
       " ('de', 104),\n",
       " ('oasi', 104),\n",
       " ('lst', 104),\n",
       " ('janet', 104),\n",
       " ('off', 103),\n",
       " ('appreci', 103),\n",
       " ('dealer', 103),\n",
       " ('train', 103),\n",
       " ('verifi', 103),\n",
       " ('health', 102),\n",
       " ('stack', 102),\n",
       " ('other', 102),\n",
       " ('dy', 102),\n",
       " ('riley', 102),\n",
       " ('without', 102),\n",
       " ('publish', 102),\n",
       " ('enrononlin', 101),\n",
       " ('h', 101),\n",
       " ('enserch', 101),\n",
       " ('stop', 101),\n",
       " ('cp', 101),\n",
       " ('mg', 100),\n",
       " ('r', 100),\n",
       " ('sandi', 100),\n",
       " ('098', 100),\n",
       " ('china', 100),\n",
       " ('34', 99),\n",
       " ('remain', 99),\n",
       " ('control', 99),\n",
       " ('cut', 99),\n",
       " ('stella', 99),\n",
       " ('advis', 99),\n",
       " ('tri', 98),\n",
       " ('830', 98),\n",
       " ('record', 98),\n",
       " ('termin', 98),\n",
       " ('cernosek', 98),\n",
       " ('today', 98),\n",
       " ('phone', 98),\n",
       " ('add', 98),\n",
       " ('watch', 97),\n",
       " ('sorri', 97),\n",
       " ('those', 97),\n",
       " ('2005', 97),\n",
       " ('ms', 97),\n",
       " ('pain', 97),\n",
       " ('storag', 97),\n",
       " ('aepin', 97),\n",
       " ('dollar', 97),\n",
       " ('find', 97),\n",
       " ('miss', 97),\n",
       " ('investor', 97),\n",
       " ('lp', 97),\n",
       " ('featur', 97),\n",
       " ('853', 97),\n",
       " ('stranger', 96),\n",
       " ('thousand', 96),\n",
       " ('trader', 96),\n",
       " ('hall', 96),\n",
       " ('ship', 96),\n",
       " ('statu', 96),\n",
       " ('dow', 96),\n",
       " ('submit', 95),\n",
       " ('teja', 95),\n",
       " ('balanc', 95),\n",
       " ('kevin', 95),\n",
       " ('mcf', 95),\n",
       " ('brand', 95),\n",
       " ('year', 95),\n",
       " ('el', 95),\n",
       " ('impact', 95),\n",
       " ('saturday', 94),\n",
       " ('53', 94),\n",
       " ('notifi', 94),\n",
       " ('y', 94),\n",
       " ('most', 93),\n",
       " ('concern', 93),\n",
       " ('east', 93),\n",
       " ('public', 93),\n",
       " ('prior', 93),\n",
       " ('page', 92),\n",
       " ('basi', 92),\n",
       " ('bellami', 92),\n",
       " ('event', 92),\n",
       " ('biz', 92),\n",
       " ('old', 92),\n",
       " ('repli', 92),\n",
       " ('mm', 92),\n",
       " ('doctor', 92),\n",
       " ('apach', 92),\n",
       " ('arrang', 92),\n",
       " ('interconnect', 92),\n",
       " ('wither', 92),\n",
       " ('account', 92),\n",
       " ('understand', 91),\n",
       " ('gif', 91),\n",
       " ('determin', 91),\n",
       " ('pay', 91),\n",
       " ('doe', 91),\n",
       " ('morri', 91),\n",
       " ('target', 90),\n",
       " ('42', 90),\n",
       " ('avail', 90),\n",
       " ('later', 90),\n",
       " ('deliv', 90),\n",
       " ('error', 90),\n",
       " ('boa', 90),\n",
       " ('35', 89),\n",
       " ('password', 89),\n",
       " ('spoke', 89),\n",
       " ('look', 89),\n",
       " ('ali', 89),\n",
       " ('past', 89),\n",
       " ('prepar', 88),\n",
       " ('avila', 88),\n",
       " ('easttexa', 88),\n",
       " ('yet', 88),\n",
       " ('claim', 88),\n",
       " ('strong', 88),\n",
       " ('project', 88),\n",
       " ('bank', 88),\n",
       " ('58', 88),\n",
       " ('tax', 88),\n",
       " ('superti', 88),\n",
       " ('f', 88),\n",
       " ('titl', 88),\n",
       " ('spam', 88),\n",
       " ('someth', 88),\n",
       " ('copi', 87),\n",
       " ('link', 87),\n",
       " ('hesco', 87),\n",
       " ('els', 87),\n",
       " ('cdnow', 87),\n",
       " ('lamadrid', 87),\n",
       " ('mccoy', 87),\n",
       " ('site', 86),\n",
       " ('form', 86),\n",
       " ('mailto', 86),\n",
       " ('gpgfin', 86),\n",
       " ('hakemack', 86),\n",
       " ('herrera', 86),\n",
       " ('person', 86),\n",
       " ('bruce', 86),\n",
       " ('central', 85),\n",
       " ('sever', 85),\n",
       " ('47', 85),\n",
       " ('guarante', 85),\n",
       " ('fund', 85),\n",
       " ('weight', 85),\n",
       " ('her', 85),\n",
       " ('nguyen', 85),\n",
       " ('physic', 84),\n",
       " ('paso', 84),\n",
       " ('info', 84),\n",
       " ('walker', 84),\n",
       " ('profil', 84),\n",
       " ('120', 84),\n",
       " ('33', 84),\n",
       " ('elizabeth', 84),\n",
       " ('cs', 84),\n",
       " ('line', 84),\n",
       " ('45', 84),\n",
       " ('comment', 83),\n",
       " ('tammi', 83),\n",
       " ('lone', 83),\n",
       " ('n', 83),\n",
       " ('st', 83),\n",
       " ('such', 83),\n",
       " ('bodi', 82),\n",
       " ('vacat', 82),\n",
       " ('mid', 82),\n",
       " ('probabl', 82),\n",
       " ('eileen', 82),\n",
       " ('unit', 81),\n",
       " ('market', 81),\n",
       " ('reader', 81),\n",
       " ('kc', 81),\n",
       " ('mx', 81),\n",
       " ('beverli', 81),\n",
       " ('monthli', 81),\n",
       " ('close', 81),\n",
       " ('allow', 81),\n",
       " ('internet', 80),\n",
       " ('provid', 80),\n",
       " ('photoshop', 80),\n",
       " ('4179', 80),\n",
       " ('salli', 80),\n",
       " ('charlott', 80),\n",
       " ('img', 80),\n",
       " ('tina', 80),\n",
       " ('draft', 80),\n",
       " ('reliantenergi', 80),\n",
       " ('pharmaci', 80),\n",
       " ('corpor', 79),\n",
       " ('retail', 79),\n",
       " ('g', 79),\n",
       " ('oct', 79),\n",
       " ('receipt', 79),\n",
       " ('valign', 79),\n",
       " ('run', 78),\n",
       " ('expect', 78),\n",
       " ('nov', 78),\n",
       " ('expedia', 78),\n",
       " ('124', 78),\n",
       " ('park', 78),\n",
       " ('345', 78),\n",
       " ('9497', 78),\n",
       " ('anyon', 78),\n",
       " ('971', 78),\n",
       " ('qualiti', 78),\n",
       " ('appear', 78),\n",
       " ('life', 77),\n",
       " ('return', 77),\n",
       " ('46', 77),\n",
       " ('ponton', 77),\n",
       " ('him', 77),\n",
       " ('cheap', 77),\n",
       " ('uncertainti', 77),\n",
       " ('enw', 77),\n",
       " ('gome', 77),\n",
       " ('earl', 76),\n",
       " ('own', 76),\n",
       " ('kimberli', 76),\n",
       " ('document', 76),\n",
       " ('enerfin', 76),\n",
       " ('intrast', 76),\n",
       " ('us', 76),\n",
       " ('sex', 76),\n",
       " ('1266', 76),\n",
       " ('rebecca', 76),\n",
       " ('togeth', 76),\n",
       " ('star', 75),\n",
       " ('assum', 75),\n",
       " ('hub', 75),\n",
       " ('000000', 75),\n",
       " ('dynegi', 75),\n",
       " ('neon', 75),\n",
       " ('div', 74),\n",
       " ('btu', 74),\n",
       " ('men', 74),\n",
       " ('london', 74),\n",
       " ('duti', 74),\n",
       " ('215', 74),\n",
       " ('phillip', 74),\n",
       " ('portfolio', 74),\n",
       " ('everyon', 74),\n",
       " ('associ', 74),\n",
       " ('terri', 74),\n",
       " ('rc', 74),\n",
       " ('lose', 74),\n",
       " ('tisdal', 74),\n",
       " ('38', 73),\n",
       " ('bridg', 73),\n",
       " ('39', 73),\n",
       " ('hill', 73),\n",
       " ('hotlist', 73),\n",
       " ('anticip', 73),\n",
       " ('mop', 73),\n",
       " ('weekend', 73),\n",
       " ('moopid', 73),\n",
       " ('asset', 73),\n",
       " ('tetco', 73),\n",
       " ('johnson', 72),\n",
       " ('57', 72),\n",
       " ('mckay', 72),\n",
       " ('700', 72),\n",
       " ('christi', 72),\n",
       " ('type', 72),\n",
       " ('thought', 72),\n",
       " ('neuweil', 71),\n",
       " ('pool', 71),\n",
       " ('payment', 71),\n",
       " ('copano', 71),\n",
       " ('function', 71),\n",
       " ('vol', 71),\n",
       " ('left', 71),\n",
       " ('section', 70),\n",
       " ('transfer', 70),\n",
       " ('action', 70),\n",
       " ('eric', 70),\n",
       " ('alreadi', 70),\n",
       " ('gulf', 70),\n",
       " ('charlen', 70),\n",
       " ('super', 70),\n",
       " ('analyst', 69),\n",
       " ('complianc', 69),\n",
       " ('gtc', 69),\n",
       " ('0435', 69),\n",
       " ('012', 69),\n",
       " ('rick', 68),\n",
       " ('relat', 68),\n",
       " ('olsen', 68),\n",
       " ('answer', 68),\n",
       " ('htmlimg', 68),\n",
       " ('kelli', 68),\n",
       " ('commun', 68),\n",
       " ('gr', 68),\n",
       " ('heidi', 68),\n",
       " ('seek', 68),\n",
       " ('full', 68),\n",
       " ('holm', 68),\n",
       " ('epson', 68),\n",
       " ('b', 68),\n",
       " ('jill', 68),\n",
       " ('panenergi', 68),\n",
       " ('tufco', 68),\n",
       " ('51', 68),\n",
       " ('xanax', 68),\n",
       " ('print', 67),\n",
       " ('valadez', 67),\n",
       " ('becaus', 67),\n",
       " ('colspan', 67),\n",
       " ('role', 67),\n",
       " ('import', 67),\n",
       " ('c', 67),\n",
       " ('mr', 67),\n",
       " ('dan', 67),\n",
       " ('easi', 67),\n",
       " ('greg', 66),\n",
       " ('way', 66),\n",
       " ('famili', 66),\n",
       " ('around', 66),\n",
       " ('41', 66),\n",
       " ('king', 66),\n",
       " ('yourself', 66),\n",
       " ('p', 66),\n",
       " ('chri', 66),\n",
       " ('corel', 66),\n",
       " ('rolex', 65),\n",
       " ('onli', 65),\n",
       " ('countri', 65),\n",
       " ('direct', 65),\n",
       " ('chad', 65),\n",
       " ('daniel', 65),\n",
       " ('capac', 65),\n",
       " ('macromedia', 65),\n",
       " ('77002', 65),\n",
       " ('asap', 65),\n",
       " ('approxim', 65),\n",
       " ('charg', 65),\n",
       " ('knle', 64),\n",
       " ('valium', 64),\n",
       " ('therefor', 64),\n",
       " ('56', 64),\n",
       " ('pinion', 64),\n",
       " ('37', 64),\n",
       " ('stephen', 64),\n",
       " ('cass', 64),\n",
       " ('svc', 64),\n",
       " ('brazo', 63),\n",
       " ('assist', 63),\n",
       " ('financ', 63),\n",
       " ('unaccount', 63),\n",
       " ('privat', 63),\n",
       " ('ua', 63),\n",
       " ('dth', 63),\n",
       " ('export', 63),\n",
       " ('resum', 63),\n",
       " ('linda', 63),\n",
       " ('staff', 63),\n",
       " ('told', 62),\n",
       " ('guy', 62),\n",
       " ('winfre', 62),\n",
       " ...]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_one_bag = embed_one(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 1,\n",
       " '2000': 2,\n",
       " '37159': 1,\n",
       " 'a': 3,\n",
       " 'account': 7,\n",
       " 'add': 1,\n",
       " 'advanc': 1,\n",
       " 'all': 1,\n",
       " 'alloc': 3,\n",
       " 'allow': 1,\n",
       " 'an': 1,\n",
       " 'and': 7,\n",
       " 'ani': 2,\n",
       " 'anyon': 1,\n",
       " 'area': 1,\n",
       " 'as': 4,\n",
       " 'assist': 1,\n",
       " 'at': 1,\n",
       " 'attend': 1,\n",
       " 'avail': 1,\n",
       " 'back': 1,\n",
       " 'backup': 1,\n",
       " 'be': 2,\n",
       " 'befor': 1,\n",
       " 'began': 1,\n",
       " 'believ': 1,\n",
       " 'but': 1,\n",
       " 'by': 1,\n",
       " 'call': 1,\n",
       " 'chanc': 1,\n",
       " 'commerci': 1,\n",
       " 'concern': 1,\n",
       " 'concernig': 1,\n",
       " 'confirm': 1,\n",
       " 'continu': 2,\n",
       " 'contract': 3,\n",
       " 'cooper': 1,\n",
       " 'coordin': 1,\n",
       " 'critic': 1,\n",
       " 'deal': 1,\n",
       " 'discuss': 1,\n",
       " 'distribut': 1,\n",
       " 'dure': 1,\n",
       " 'each': 2,\n",
       " 'earli': 1,\n",
       " 'effect': 1,\n",
       " 'email': 1,\n",
       " 'encourag': 1,\n",
       " 'entex': 6,\n",
       " 'especi': 1,\n",
       " 'etc': 1,\n",
       " 'everyon': 2,\n",
       " 'feel': 1,\n",
       " 'few': 2,\n",
       " 'final': 1,\n",
       " 'first': 1,\n",
       " 'for': 4,\n",
       " 'free': 1,\n",
       " 'from': 3,\n",
       " 'front': 1,\n",
       " 'get': 1,\n",
       " 'give': 1,\n",
       " 'group': 2,\n",
       " 'ha': 1,\n",
       " 'hard': 1,\n",
       " 'have': 1,\n",
       " 'held': 1,\n",
       " 'hi': 1,\n",
       " 'hope': 1,\n",
       " 'howard': 3,\n",
       " 'i': 4,\n",
       " 'identifi': 1,\n",
       " 'if': 2,\n",
       " 'in': 4,\n",
       " 'initi': 3,\n",
       " 'is': 2,\n",
       " 'issu': 2,\n",
       " 'januari': 1,\n",
       " 'juli': 1,\n",
       " 'kickoff': 1,\n",
       " 'lead': 1,\n",
       " 'learn': 2,\n",
       " 'like': 1,\n",
       " 'list': 1,\n",
       " 'manag': 2,\n",
       " 'me': 1,\n",
       " 'meet': 4,\n",
       " 'member': 2,\n",
       " 'month': 4,\n",
       " 'my': 1,\n",
       " 'necessari': 1,\n",
       " 'newcom': 1,\n",
       " 'next': 1,\n",
       " 'nguyen': 1,\n",
       " 'not': 1,\n",
       " 'of': 5,\n",
       " 'offic': 2,\n",
       " 'on': 3,\n",
       " 'onli': 1,\n",
       " 'or': 3,\n",
       " 'organ': 1,\n",
       " 'other': 1,\n",
       " 'over': 2,\n",
       " 'part': 1,\n",
       " 'phase': 1,\n",
       " 'pleas': 2,\n",
       " 'point': 1,\n",
       " 'primari': 1,\n",
       " 'process': 2,\n",
       " 'purpos': 1,\n",
       " 'question': 2,\n",
       " 'recap': 1,\n",
       " 'rel': 1,\n",
       " 'relat': 1,\n",
       " 'resolv': 1,\n",
       " 'respect': 1,\n",
       " 'respons': 1,\n",
       " 'responsibilit': 1,\n",
       " 's': 2,\n",
       " 'schedul': 1,\n",
       " 'settlement': 1,\n",
       " 'she': 1,\n",
       " 'some': 2,\n",
       " 'speed': 1,\n",
       " 'stop': 1,\n",
       " 'subject': 1,\n",
       " 'success': 1,\n",
       " 'take': 1,\n",
       " 'team': 2,\n",
       " 'thank': 2,\n",
       " 'the': 24,\n",
       " 'these': 2,\n",
       " 'thi': 5,\n",
       " 'thu': 4,\n",
       " 'to': 16,\n",
       " 'train': 1,\n",
       " 'transist': 2,\n",
       " 'transit': 1,\n",
       " 'up': 1,\n",
       " 'volum': 2,\n",
       " 'well': 1,\n",
       " 'will': 6,\n",
       " 'with': 2,\n",
       " 'within': 1,\n",
       " 'work': 1,\n",
       " 'would': 3,\n",
       " 'x': 1,\n",
       " 'year': 1,\n",
       " 'yesterday': 1,\n",
       " 'you': 1,\n",
       " 'your': 2}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_one_bag[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'christma', 'tree', 'farm', 'pictur'])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_one_bag[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_dicts= diff_dicts[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ect_count, ect_mid = median_cut(whole_one_bag, top_dicts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_cut(embedded_dict, keyword):\n",
    "    \"\"\"\n",
    "    Cut the whole thing into median\n",
    "    \"\"\"\n",
    "    ndict = len(embedded_dict)\n",
    "    count = np.zeros(ndict)\n",
    "    #count = np.zeros(10)\n",
    "    for i in range(ndict):\n",
    "        onekeys = embedded_dict[i].keys()\n",
    "        if keyword in onekeys:\n",
    "            #print (embedded_dict[i][keyword])\n",
    "            count[i] = embedded_dict[i][keyword]\n",
    "            \n",
    "        else:\n",
    "            count[i] = 0\n",
    "            \n",
    "    return count, np.median(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), 0.0)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_cut(spam_one_bag, top_dicts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4279.,   108.,    58.,    83.,    41.,    67.,    40.,    45.,\n",
       "           61.]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " <a list of 9 Patch objects>)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD9CAYAAAC1DKAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a160f4a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ect_count, bins=np.arange(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4279"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(ect_count == 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((5,2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decision_threshold(train1, train2, keyword):\n",
    "    \"\"\"\n",
    "    Compute the entropy to determine the decision boundary\n",
    "    \"\"\"\n",
    "    train1_distr, train1_med = median_cut(train1, keyword)\n",
    "    train2_distr, train2_med = median_cut(train2, keyword)\n",
    "    \n",
    "    maxi = int(np.max(np.concatenate((train1_distr,train2_distr))))\n",
    "    \n",
    "    entropies = np.zeros((maxi, 2))\n",
    "    weighted_entrop = np.zeros(maxi)\n",
    "    #print (np.sum(train1_distr)+np.sum(train2_distr))\n",
    "    print (maxi)\n",
    "    for i in range(maxi):\n",
    "        ntrain1_low = len(np.where(train1_distr <= i)[0])\n",
    "        ntrain2_low = len(np.where(train2_distr <= i)[0])\n",
    "        entropies[i,0] = compute_entropy(ntrain1_low, ntrain2_low)\n",
    "        \n",
    "        ntrain1_high = len(np.where(train1_distr > i)[0])\n",
    "        ntrain2_high = len(np.where(train2_distr > i)[0])\n",
    "        entropies[i,1] = compute_entropy(ntrain1_high, ntrain2_high)\n",
    "        \n",
    "        ntrain_low = ntrain1_low + ntrain2_low\n",
    "        ntrain_high = ntrain1_high + ntrain2_high\n",
    "        all_ntrain = ntrain_low + ntrain_high\n",
    "    \n",
    "        weighted_entrop[i] = ntrain_low / all_ntrain * entropies[i,0] + ntrain_high / all_ntrain * entropies[i,1]\n",
    "        \n",
    "    return np.nanargmin(weighted_entrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Doyeon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log2\n",
      "  \n",
      "/Users/Doyeon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "a_low, a_high, b_low, b_high = Decision_Tree(spam_one_bag, ham_one_bag, top_dicts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_low) + len(a_high) + len(b_low) + len(b_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DTnode(object):\n",
    "    def __init__(self, threshold, major, feature, left, right):\n",
    "        self.threshold = threshold\n",
    "        self.feature = feature\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.major = major\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Make_DT(train1, train2, featurelist):\n",
    "    if len(train1) > len(train2):\n",
    "        major = 0.\n",
    "    else:\n",
    "        major = 1.    \n",
    "    \n",
    "    #if len(featurelist) == 0:\n",
    "    \n",
    "    feature = featurelist[0]\n",
    "    thresh = find_decision_threshold(train1, train2, feature)\n",
    "    \n",
    "    low1, high1, low2, high2 = spliting_sets(train1, train2, feature)\n",
    "    \n",
    "    \n",
    "    featurelist = featurelist[1:]\n",
    "    return_node = DTnode(threshold=thresh, major=major, feature=feature, left=None, right=None)\n",
    "    if len(featurelist) == 0:\n",
    "        return return_node\n",
    "    if \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spliting_sets(train1, train2, keyword):\n",
    "    \"\"\"\n",
    "    Spliting\n",
    "    \"\"\"\n",
    "    cut_thresh = find_decision_threshold(train1, train2, keyword)\n",
    "    train1_distr, train1_med = median_cut(train1, keyword)\n",
    "    train2_distr, train2_med = median_cut(train2, keyword)\n",
    "    \n",
    "    ntrain1 = len(train1_distr)\n",
    "    ntrain2 = len(train2_distr)\n",
    "    \n",
    "    train1_low = []\n",
    "    train1_high = []\n",
    "    train2_low = []\n",
    "    train2_high = []\n",
    "    \n",
    "    for i in range(ntrain1):\n",
    "        if train1_distr[i] <= cut_thresh:\n",
    "            train1_low.append(train1[i])\n",
    "        else:\n",
    "            train1_high.append(train1[i])\n",
    "        \n",
    "    for j in range(ntrain2):\n",
    "        if train2_distr[i] <= cut_thresh:\n",
    "            train2_low.append(train2[i])\n",
    "        else:\n",
    "            train2_high.append(train2[i])\n",
    "    \n",
    "    return train1_low, train1_high, train2_low, train2_high\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(train1_count, train2_count):\n",
    "    \"\"\"\n",
    "    Compute Entropy to judge whether this is a good classifier for one dictionary\n",
    "    \"\"\"\n",
    "    if any(train1_count)\n",
    "    total_count = train1_count + train2_count\n",
    "    \n",
    "    ratio1 = train1_count / total_count\n",
    "    ratio2 = train2_count / total_count\n",
    "    entropy = - ratio1 * np.log2(ratio1) - ratio2 * np.log2(ratio2)\n",
    "    return entropy\n",
    "    \n",
    "\n",
    "def Decision_Tree(test, train1, train2, entropy_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Compute Entropy of test which overlaps with the training sets with Decision Tree\n",
    "    \n",
    "    Input \n",
    "    test = ['email message']\n",
    "    train1&2 [dict] : a dictionary of total bag-of-words\n",
    "    Returns to entropy\n",
    "    \"\"\"\n",
    "    testing = embed_one(test)[0]\n",
    "    testkeys = testing.keys()\n",
    "    train1_keys = train1.keys()\n",
    "    train2_keys = train2.keys()\n",
    "    merged_keys = (set(train1_keys) | set(train2_keys))\n",
    "    #testkeys_list = list(testkeys)\n",
    "    ntest = len(testkeys)\n",
    "    tk1_only = list(set(train1_keys)-set(train2_keys))\n",
    "    tk2_only = list(set(train2_keys)-set(train1_keys))\n",
    "    all_train = np.vstack((train1, train2))\n",
    "    \n",
    "    weights = np.ones(2)\n",
    "\n",
    "    for tk in testkeys:\n",
    "        if tk in tk1_only:\n",
    "            weights[0] *= 1000\n",
    "        elif tk in tk2_only:\n",
    "            weights[1] *= 1000\n",
    "        elif tk not in merged_keys:\n",
    "            pass\n",
    "        else:\n",
    "            entropy = compute_entropy(train1[tk], train2[tk])\n",
    "            if entropy >= entropy_thresh:\n",
    "                #print (testing[tk], train1[tk], train2[tk])\n",
    "                weights[0] *= testing[tk] / train1[tk]\n",
    "                weights[1] *= testing[tk] / train2[tk]  \n",
    "                \n",
    "    return weights \n",
    "    \n",
    "\n",
    "def Naive_Bayes(test, train, prior=1500/5172):\n",
    "    \"\"\"\n",
    "    Compute the score of test with respect to training sets using Naive Bayes\n",
    "    \n",
    "    Input\n",
    "    test = ['email message']\n",
    "    train [dict] : a dictionary of total bag-of-words\n",
    "    prior = N_email / N_total (eg. 1500/5172 for SPAM or 3672/1500 for HAM)\n",
    "    Returns to scores\n",
    "    \"\"\"\n",
    "    train_keys = train.keys()\n",
    "    all_train_vals = sum(list(train.values())) ## sum of all occurances\n",
    "    \n",
    "    ### find the overlapping term\n",
    "    testing = embed_one(test)[0]\n",
    "    testkeys = testing.keys()\n",
    "    ntest = len(testkeys)\n",
    "    \n",
    "    score = prior\n",
    "    \n",
    "    for i in range(ntest):\n",
    "        denominator = (all_train_vals + ntest+1)\n",
    "        tkey = list(testkeys)[i]\n",
    "        if tkey in train_keys:\n",
    "            multi_factor = testing[tkey]\n",
    "            score *= ((train[tkey] + 1) / denominator)**(multi_factor)\n",
    "        else:\n",
    "            score *= 1 / denominator\n",
    "            \n",
    "    return score\n",
    "\n",
    "\n",
    "def NN_Distances(test, train, nn_option='L2'):\n",
    "    \"\"\"\n",
    "    Compute the distance of test based on training sets using Nearest Neighbor\n",
    "    test: test [one_email] ; train (individual dictionaries)\n",
    "    \n",
    "    Input:\n",
    "    test = ['email message']\n",
    "    train [list] : a collection of individual set dict\n",
    "    \n",
    "    Returns to the array of distances\n",
    "    \"\"\"\n",
    "    testing = embed_one(test)[0] ## because it's just one\n",
    "    testkeys = testing.keys()\n",
    "    ntest = len(testkeys)\n",
    "\n",
    "    ntrain = len(train)\n",
    "    dist = np.zeros(ntrain)\n",
    "    \n",
    "    for i in range(ntrain):\n",
    "        onekey = train[i].keys()\n",
    "        one_eval = testing.copy()\n",
    "        one_eval.update(train[i]) ## for all un-matching dictionaries\n",
    "        for j in range(ntest):\n",
    "            if list(testkeys)[j] in onekey:\n",
    "                thiskey = list(testkeys)[j]\n",
    "                one_eval[thiskey] = train[i][thiskey] - testing[thiskey] ## subtract only when items are matching\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        one_eval = np.array(list(one_eval.values()))\n",
    "        #print (one_eval)\n",
    "        if nn_option == 'L1' or nn_option == 'Linf':\n",
    "            dist[i] = sum(np.abs(one_eval))\n",
    "        elif nn_option == 'L2':\n",
    "            dist[i] = np.sqrt(sum(one_eval**2))\n",
    "            \n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16.58312395,  10.95445115,   6.92820323, ...,   5.74456265,\n",
       "         5.74456265,  16.37070554])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Distances(phrase, spam_one_bag, nn_option='L2') ## Returns to distance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8051631976513072e-22"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Naive_Bayes(phrase, ham_sum_bag) ## Returns to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.35642591e-07,   4.74802019e-07])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decision_Tree(phrase, ham_sum_bag, spam_sum_bag, entropy_thresh=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: meter 1431 - nov 1999\\ndaren -\\ncould you please resolve this issue for howard ? i will be out of the office\\nthe next two days .\\nwhen this is done , please let george know . thanks .\\naimee\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by aimee lannou / hou / ect on 12 / 15 / 99 01 : 27 pm\\n- - - - - - - - - - - - - - - - - - - - - - - - - - -\\nhoward b camp\\n12 / 15 / 99 01 : 01 pm\\nto : aimee lannou / hou / ect @ ect\\ncc : daren j farmer / hou / ect @ ect , stacey neuweiler / hou / ect @ ect , mary m\\nsmith / hou / ect @ ect\\nsubject : meter 1431 - nov 1999\\naimee ,\\nsitara deal 92943 for meter 1431 has expired on oct 31 , 1999 . settlements\\nis unable to draft an invoice for this deal . this deal either needs to be\\nextended or a new deal needs to be set up . please let me know when this is\\nresolved . we need it resolved by friday , dec 17 .\\nhc'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_testing[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-3f2e2a2137e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_Distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_testing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mham_sum_bag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-134-d5098d415060>\u001b[0m in \u001b[0;36mNN_Distances\u001b[0;34m(test, train, nn_option)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0monekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mone_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mone_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## for all un-matching dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "NN_Distances(ham_testing[10], ham_sum_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns to a common measure\n",
    "def Classifier(new, one, two, option='NB', nn_option='L2'):\n",
    "    \"\"\"\n",
    "    For \"overlapping\" bag of words between new and training sets, evaluate probability based on classifier of choice\n",
    "    NB: Naive Bayes; DT: Decision Tree; NN: Nearest Neighbors\n",
    "    New (to-be-examined); one(training 1); two(training 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    ntest = len(new)\n",
    "    scores = np.zeros((ntest,2))\n",
    "    if option == 'NN':\n",
    "        one_bag = embed_one(one) \n",
    "        two_bag = embed_one(two)\n",
    "    else:\n",
    "        one_bag = embed_whole(one)\n",
    "        two_bag = embed_whole(two)\n",
    "        \n",
    "    for n in range(ntest):\n",
    "        if option == 'NN':\n",
    "            dist_one = NN_Distances(new[n], one_bag, nn_option)\n",
    "            dist_two = NN_Distances(new[n], two_bag, nn_option)\n",
    "            if nn_option == 'L1' or nn_option == 'L2':\n",
    "                scores[n,0] = 1 / np.sum(dist_one) ## only for NN smaller number signifies better score\n",
    "                scores[n,1] = 1 / np.sum(dist_two)\n",
    "            elif nn_option == 'Linf':\n",
    "                scores[n,0] = np.max(dist_one)\n",
    "                scores[n,1] = np.max(dist_two)\n",
    "\n",
    "        elif option == 'NB':\n",
    "            scores[n,0] = Naive_Bayes(new[n], one_bag)\n",
    "            scores[n,1] = Naive_Bayes(new[n], two_bag)\n",
    "\n",
    "        elif option == 'DT':\n",
    "            scores[n] = Decision_Tree(new[n], one_bag, two_bag, entropy_thresh=0.8)\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((8,2))[7,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TESTING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Doyeon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: This function is deprecated. Please call randint(0, 3500 + 1) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/Doyeon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: This function is deprecated. Please call randint(0, 1499 + 1) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "random_draw0 = np.random.random_integers(0,3500,1000)\n",
    "ramdom_draw1 = np.random.random_integers(0,1499,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hams_array = np.array(hams.copy())\n",
    "spams_array = np.array(spams.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test_index = np.array(list(set(np.arange(len(hams)))- set(random_draw0))[:100])\n",
    "spam_test_index = np.array(list(set(np.arange(len(spams)))- set(ramdom_draw1))[:100])\n",
    "\n",
    "ham_testing = hams_array[[ham_test_index]]\n",
    "spam_testing = spams_array[[spam_test_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_training = hams_array[[random_draw0]]\n",
    "spam_training = spams_array[[ramdom_draw1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NB', nn_option='L2')\n",
    "nb_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NB', nn_option='L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00019363  0.00016005] [ 0.00019363  0.00016005]\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(nb_scores_ham, axis=0), np.sum(nb_scores_spam, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnl1_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NN', nn_option='L1')\n",
    "nnl1_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NN', nn_option='L1')\n",
    "\n",
    "nnl2_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NN', nn_option='L2')\n",
    "nnl2_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NN', nn_option='L2')\n",
    "\n",
    "nnlinf_scores_ham = Classifier(ham_testing, ham_training, spam_training, option='NN', nn_option='Linf')\n",
    "nnlinf_scores_spam = Classifier(spam_testing, ham_training, spam_training, option='NN', nn_option='Linf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0006632   0.00054884] [ 0.0006632   0.00054884]\n",
      "[ 0.00489144  0.00479541] [ 0.00489144  0.00479541]\n",
      "[ 275800.  233400.] [ 275800.  233400.]\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(nnl1_scores_ham, axis=0), np.sum(nnl1_scores_spam, axis=0))\n",
    "print (np.sum(nnl2_scores_ham, axis=0), np.sum(nnl2_scores_spam, axis=0))\n",
    "print (np.sum(nnlinf_scores_ham, axis=0), np.sum(nnlinf_scores_spam, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### only evaluate the overlapping keywords?\n",
    "inter_spam, inter_ham = overlap(spam_bag, ham_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INCORRECT (Without Stemming)\n",
    "def embed_whole(datalist):\n",
    "    \"\"\"\n",
    "    From a list of data (should have multiple), do stemming (+remove non-words) then apply the bag-of-words model\n",
    "    \n",
    "    Returns\n",
    "    a dictionary of bag-of-words each dic corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag = {}\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = tokenizer.tokenize(datalist[i])\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in bag:\n",
    "                bag[key] += 1\n",
    "            else:\n",
    "                bag[key] = 1        \n",
    "    return bag\n",
    "\n",
    "def embed_one(datalist):\n",
    "    \"\"\"\n",
    "    Construct stemmed+bag-of-words model for individual then construct an array of individual bags\n",
    "    \n",
    "    Returns\n",
    "    a collection of individual set dict corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag_collection = []\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        one_bag = {}\n",
    "        stemmed = tokenizer.tokenize(datalist[i])\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in one_bag:\n",
    "                one_bag[key] += 1\n",
    "            else:\n",
    "                one_bag[key] = 1\n",
    "        bag_collection.append(one_bag)\n",
    "        \n",
    "    return bag_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
