{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ham_address = '/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/ham/'\n",
    "spam_address = '/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/spam/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_file = open('/Volumes/Extra/Columbia/Fall2018/Classes/ML/enron1/Summary.txt')\n",
    "summary = summary_file.read()\n",
    "summary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hams = []\n",
    "hamfiles = sorted(glob.glob(ham_address+'*.txt'))\n",
    "hfiles = len(hamfiles)\n",
    "for i in range(hfiles):\n",
    "    file = open(hamfiles[i], 'rt')\n",
    "    text = file.read()\n",
    "    hams.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spams = []\n",
    "spamfiles = sorted(glob.glob(spam_address+'*.txt'))\n",
    "sfiles = len(spamfiles)\n",
    "for i in range(sfiles):\n",
    "    file = open(spamfiles[i], 'rt', encoding=\"latin-1\")\n",
    "    text = file.read()\n",
    "    spams.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "ps = PorterStemmer()\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_one(datalist):\n",
    "    \"\"\"\n",
    "    Construct stemmed+bag-of-words model for individual then construct an array of individual bags\n",
    "    \n",
    "    Returns\n",
    "    a collection of individual set dict corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag_collection = []\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = []\n",
    "        token = tokenizer.tokenize(datalist[i])\n",
    "        for w in token:\n",
    "            stemmed.append(ps.stem(w))\n",
    "    \n",
    "        nstem = len(stemmed)\n",
    "        one_bag = {}\n",
    "        \n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in one_bag:\n",
    "                one_bag[key] += 1\n",
    "            else:\n",
    "                one_bag[key] = 1\n",
    "        bag_collection.append(one_bag)\n",
    "        \n",
    "    return bag_collection\n",
    "\n",
    "def embed_whole(datalist):\n",
    "    \"\"\"\n",
    "    From a list of data (should have multiple), do stemming (+remove non-words) then apply the bag-of-words model\n",
    "    \n",
    "    Returns\n",
    "    a dictionary of bag-of-words each dic corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag = {}\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = []\n",
    "        token = tokenizer.tokenize(datalist[i])\n",
    "        for w in token:\n",
    "            stemmed.append(ps.stem(w))\n",
    "        #stemmed = list(set(stemmed))\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in bag:\n",
    "                bag[key] += 1\n",
    "            else:\n",
    "                bag[key] = 1        \n",
    "    return bag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_one_bag = embed_one(hams)\n",
    "spam_one_bag = embed_one(spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = np.concatenate((hams, spams))\n",
    "whole_bag = embed_whole(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_sum_bag = embed_whole(spams)\n",
    "ham_sum_bag = embed_whole(hams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Find the overlapping dictionaries\n",
    "    \"\"\"\n",
    "    key1 = set(dict1.keys())\n",
    "    key2 = set(dict2.keys())\n",
    "    intersection = key1 & key2\n",
    "    newdict1 = {}\n",
    "    newdict2 = {}\n",
    "    intersection = list(intersection)\n",
    "    ninter = len(intersection)\n",
    "    for i in range(ninter):\n",
    "        newdict1[intersection[i]] = dict1[intersection[i]]\n",
    "        newdict2[intersection[i]] = dict2[intersection[i]]\n",
    "    return newdict1, newdict2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovlp_spam, ovlp_ham = overlap(spam_sum_bag, ham_sum_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrase = ['I likes you like you tree']\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "words=tokenizer.tokenize(phrase[0])\n",
    "emp = []\n",
    "for w in words:\n",
    "    emp.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'you', 'like', 'you', 'tree']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embed_one(phrase)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree(test, train):\n",
    "    \"\"\"\n",
    "    Compute Entropy of test which overlaps with the training sets with Decision Tree\n",
    "    \n",
    "    Returns to entropy\n",
    "    \"\"\"\n",
    "    return None\n",
    "    \n",
    "    \n",
    "\n",
    "def Naive_Bayes(test, train, prior=1500/5172):\n",
    "    \"\"\"\n",
    "    Compute the score of test with respect to training sets using Naive Bayes\n",
    "    \n",
    "    Input\n",
    "    test = ['email message']\n",
    "    train [dict] : a dictionary of total bag-of-words\n",
    "    prior = N_email / N_total (eg. 1500/5172 for SPAM or 3672/1500 for HAM)\n",
    "    Returns to scores\n",
    "    \"\"\"\n",
    "    train_keys = train.keys()\n",
    "    all_train_vals = sum(list(train.values())) ## sum of all occurances\n",
    "    \n",
    "    ### find the overlapping term\n",
    "    testing = embed_one(test)[0]\n",
    "    testkeys = testing.keys()\n",
    "    ntest = len(testkeys)\n",
    "    \n",
    "    score = prior\n",
    "    \n",
    "    for i in range(ntest):\n",
    "        denominator = (all_train_vals + ntest+1)\n",
    "        tkey = list(testkeys)[i]\n",
    "        if tkey in train_keys:\n",
    "            multi_factor = testing[tkey]\n",
    "            score *= ((train[tkey] + 1) / denominator)**(multi_factor)\n",
    "        else:\n",
    "            score *= 1 / denominator\n",
    "            \n",
    "    return score\n",
    "\n",
    "\n",
    "def NN_Distances(test, train, nn_option='L2'):\n",
    "    \"\"\"\n",
    "    Compute the distance of test based on training sets using Nearest Neighbor\n",
    "    test: test [one_email] ; train (individual dictionaries)\n",
    "    \n",
    "    Input:\n",
    "    test = ['email message']\n",
    "    train [list] : a collection of individual set dict\n",
    "    \n",
    "    Returns to the array of distances\n",
    "    \"\"\"\n",
    "    testing = embed_one(test)[0] ## because it's just one\n",
    "    testkeys = testing.keys()\n",
    "    ntest = len(testkeys)\n",
    "\n",
    "    ntrain = len(train)\n",
    "    dist = np.zeros(ntrain)\n",
    "    \n",
    "    for i in range(ntrain):\n",
    "        onekey = train[i].keys()\n",
    "        one_eval = testing.copy()\n",
    "        one_eval.update(train[i]) ## for all un-matching dictionaries\n",
    "        for j in range(ntest):\n",
    "            if list(testkeys)[j] in onekey:\n",
    "                thiskey = list(testkeys)[j]\n",
    "                one_eval[thiskey] = train[i][thiskey] - testing[thiskey] ## subtract only when items are matching\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        one_eval = np.array(list(one_eval.values()))\n",
    "        #print (one_eval)\n",
    "        if nn_option == 'L1' or nn_option == 'Linf':\n",
    "            dist[i] = sum(np.abs(one_eval))\n",
    "        elif nn_option == 'L2':\n",
    "            dist[i] = np.sqrt(sum(one_eval**2))\n",
    "            \n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16.58312395,  10.95445115,   6.92820323, ...,   5.74456265,\n",
       "         5.74456265,  16.37070554])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Distances(phrase, spam_one_bag, nn_option='L2') ## Returns to distance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8051631976513072e-22"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Naive_Bayes(phrase, ham_sum_bag) ## Returns to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns to a common measure\n",
    "def Classifier(new, one, two, option='NB', nn_option='L2'):\n",
    "    \"\"\"\n",
    "    For \"overlapping\" bag of words between new and training sets, evaluate probability based on classifier of choice\n",
    "    NB: Naive Bayes; DT: Decision Tree; NN: Nearest Neighbors\n",
    "    New (to-be-examined); one(training 1); two(training 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    common_new, common_one = overlap(new, one)\n",
    "    \n",
    "    ckeys = common\n",
    "    ncommon = len(common)\n",
    "    \n",
    "    scores = np.zeros(2)\n",
    "    if option == 'NN':\n",
    "        dist_one = NN_Distances(new, one, nn_option)\n",
    "        dist_two = NN_Distances(new, two, nn_option)\n",
    "        if nn_option == 'L1' or nn_option == 'L2':\n",
    "            scores[0] = 1 / np.sum(dist_one) ## only for NN smaller number signifies better score\n",
    "            scores[1] = 1 / np.sum(dist_two)\n",
    "        elif nn_option == 'Linf':\n",
    "            scores[0] = np.max(dist_one)\n",
    "            scores[1] = np.max(dist_two)\n",
    "    \n",
    "    elif option == 'NB':\n",
    "        scores[0] = Naive_Bayes(new, one)\n",
    "        scores[1] = Naive_Bayes(new, two)\n",
    "        \n",
    "            \n",
    "    elif option == 'DT':\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### only evaluate the overlapping keywords?\n",
    "inter_spam, inter_ham = overlap(spam_bag, ham_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INCORRECT (Without Stemming)\n",
    "def embed_whole(datalist):\n",
    "    \"\"\"\n",
    "    From a list of data (should have multiple), do stemming (+remove non-words) then apply the bag-of-words model\n",
    "    \n",
    "    Returns\n",
    "    a dictionary of bag-of-words each dic corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag = {}\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        stemmed = tokenizer.tokenize(datalist[i])\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in bag:\n",
    "                bag[key] += 1\n",
    "            else:\n",
    "                bag[key] = 1        \n",
    "    return bag\n",
    "\n",
    "def embed_one(datalist):\n",
    "    \"\"\"\n",
    "    Construct stemmed+bag-of-words model for individual then construct an array of individual bags\n",
    "    \n",
    "    Returns\n",
    "    a collection of individual set dict corresponding to its counts\n",
    "    \"\"\"\n",
    "    bag_collection = []\n",
    "    ndata = len(datalist)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(ndata):\n",
    "        one_bag = {}\n",
    "        stemmed = tokenizer.tokenize(datalist[i])\n",
    "        nstem = len(stemmed)\n",
    "        for j in range(nstem):\n",
    "            key = stemmed[j]\n",
    "            if key in one_bag:\n",
    "                one_bag[key] += 1\n",
    "            else:\n",
    "                one_bag[key] = 1\n",
    "        bag_collection.append(one_bag)\n",
    "        \n",
    "    return bag_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
